{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering JGI Publications - Template\n",
    "\n",
    "Notebook intended to provide a framework for a clustering workflow to be used with titles and abstracts of JGI-enabled publications.\n",
    "\n",
    "The __major goal__ if this project is to see if we can thematically group a set of publications. Ideally, our groupings will be as close to reality as possible. If a subject matter expert (i.e. a plant biologist) were to look at your model's results, would that person agree with them or at least judge them to be coherent topics that *could* emerge from a group of papers in their domain?\n",
    "\n",
    "Text analysis is useful for impact-assessment because it allows us to more efficiently summarize / categorize the scientific activities of JGI users. If we spend a lot of money on a given project, it makes sense to ask what on earth people are actually using the resulting products for. Are there counter-intuitive uses that we didn't originally think of? Do some projects lend themselves better to use cases other than those for which they were originally created? Rather than have a scientist sort through the results, we can use text analysis to get a loose idea of this picture in relatively short order.\n",
    "\n",
    "*Note: The order below is more of a logical workflow than what actually needs to happen for your code. If you want to turn anything into standalone functions, you may want to shuffle things around. If you want to translate your code to a workable script, it will likely need to be split into various functions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Imports\n",
    "\n",
    "What packages / modules do you need? Those mentioned below are just those that I use in the current version of my workflow. Feel free to add/delete as you see fit.\n",
    "\n",
    "Most of these are probably in the standard Anaconda distribution, but you may need to add/update certain packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os - https://docs.python.org/3/library/os.html\n",
    "# for interacting with your computer's file system\n",
    "import os\n",
    "\n",
    "#-------------------------------\n",
    "# pandas - https://pandas.pydata.org/docs/user_guide/index.html\n",
    "# for structuring and shaping your data\n",
    "import pandas as pd\n",
    "# command for Jupyter to display more than the default 50 pandas rows at once. Adjust number to preference\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "#-------------------------------\n",
    "# re - https://docs.python.org/3/library/re.html\n",
    "# for building custom regular expressions for use in text pre-processing\n",
    "import re\n",
    "\n",
    "#-------------------------------\n",
    "# string - https://docs.python.org/3/library/string.html\n",
    "# for removing punctuation and other misc. string operations during pre-processing\n",
    "import string\n",
    "\n",
    "#-------------------------------\n",
    "# Natural Language Toolkit - https://www.nltk.org/\n",
    "# Expansive library for working with natural language / free text data\n",
    "\n",
    "# modules\n",
    "# Stopwords corpus - https://www.nltk.org/nltk_data/\n",
    "# Provides access to lists of common stopwords\n",
    "from nltk.corpus import stopwords as english_stopwords\n",
    "# Word tokenizer - https://www.nltk.org/api/nltk.tokenize.html\n",
    "# Breaks a string into a list of substring tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "# lematizer - https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "# Reduces tokens to their 'lemma' (base grammatical forms) as opposed to their stems (base form without affixes, igorant of grammatical context)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Part-of-speech tagger - https://www.nltk.org/api/nltk.tag.html\n",
    "# Processes a sequence of words, and attaches a part of speech tag to each word (Used as input for lemmatizer)\n",
    "from nltk import pos_tag\n",
    "# Wordnet - https://www.nltk.org/howto/wordnet.html\n",
    "# Used to convert pos_tag output into a form that can be used as input for the lemmatizing operation\n",
    "from nltk.corpus import wordnet\n",
    "# Porter Stemmer - https://www.nltk.org/_modules/nltk/stem/porter.html\n",
    "# Applies the Porter-Stemming operation to tokens. Reduces words to their base, affix-less form, but loses some morphological associations\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#-------------------------------\n",
    "# SciKit Learn - https://scikit-learn.org/stable/\n",
    "# Expansive machine learning library\n",
    "\n",
    "# modules (algorithms) :\n",
    "# Latent Dirichlet Allocation (LDA) - Not really a clustering operation. Maps topics over sets of terms, documents assigned to topics based on term-alignment with topic terms\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# other clustering algorithms here: https://scikit-learn.org/stable/modules/clustering.html#clustering\n",
    "\n",
    "# modules (utilities)\n",
    "# CountVectorizer - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# Transforms string documents into a term-frequency (TF) matrix. (Only type of input accepted by LDA)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# TFIDF Vectorizer - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "# Transforms string documents into a term-frequency inverse-document-frequency (TFIDF) matrix. Many algorithms accept both TF and TFIDF, but TFIDF is more nuanced\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sparse matrix library\n",
    "# for stopword generation\n",
    "from scipy.sparse import coo_array, csr_array, csc_array, csr_matrix, coo_matrix, csc_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "#-------------------------------\n",
    "# time - https://docs.python.org/3/library/time.html\n",
    "# for timing operations\n",
    "from time import time\n",
    "\n",
    "#-------------------------------\n",
    "# numpy - https://numpy.org/\n",
    "# for working with array formats used as input/output by certain SciKit Learn operations and accessory functions\n",
    "import numpy as np\n",
    "\n",
    "#-------------------------------\n",
    "# PyLDAvis (Latent Dirchlet Allocation visualization for Python) - https://pyldavis.readthedocs.io/en/latest/readme.html\n",
    "# Excellent library for visualizing the output of SciKit Learn LDA models\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# --------------------\n",
    "# Warnings - https://docs.python.org/3/library/warnings.html\n",
    "# Not essential. Using this function prevents deprecation warnings, etc. from cluttering your notebook.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Load data\n",
    "\n",
    "Import your source data. What columns are important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = pd.read_csv(\"all_soybean_citations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = citations[~citations.abstract.isna() & ~citations.title.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'abstract', 'altmetric', 'authors', 'category_for',\n",
       "       'category_rcdc', 'category_uoa', 'concepts_scores', 'date', 'doi',\n",
       "       'funders', 'open_access', 'pages', 'reference_ids',\n",
       "       'research_org_cities', 'research_org_countries',\n",
       "       'research_org_country_names', 'research_orgs', 'researchers',\n",
       "       'times_cited', 'type', 'volume', 'year', 'journal.id', 'journal.title',\n",
       "       'research_org_state_codes', 'research_org_state_names', 'pmcid',\n",
       "       'category_bra', 'issue', 'pmid', 'category_hrcs_hc', 'category_hra',\n",
       "       'category_hrcs_rac', 'category_sdg', 'field_citation_ratio',\n",
       "       'relative_citation_ratio', 'category_icrp_cso', 'category_icrp_ct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = citations.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the above columns will be extraneous for your purposes. All of the 'category' fields are automatically derived by the data provider I use (Dimensions), so I wouldn't try to incorporate these into my models unless you've worked with the titles + abstracts and are curious as to how these other fields might affect your results. In general, I try to avoid running machine learning operations on text results derived from other automated processes, as this can add layers of abstraction that may be difficult to explain or replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that some steps below reference the Pandas DataFrame variable called __'data'__. In my workflow, I return a copy of my original 'citations' DataFrame after my preprocessing steps, which includes a column called 'txt' to store my processed text for each document.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Pre-process text\n",
    "\n",
    "What does your text need to look like to ensure that your algorithm will provide meaningful results?\n",
    "\n",
    "__Perhaps a good first step is to ignore pre-processing altogether. Simply send your raw text to the vectorizer and see what your algorithm spits out. How does it look? What would you change?__\n",
    "\n",
    "Some possible routes to explore:\n",
    "* Noise / formatting cleanup (html tags, copyright notices, URLs, standalone punctuation/numbers, etc.)?\n",
    "* Tokenization? Case-standardization (lower-casing)?\n",
    "* Stopword removal?\n",
    "  * General English?\n",
    "  * General scientific literature? (let me know if you want me to send you my go-to list, or come up with one on your own)\n",
    "  * Soybean research: domain-specific?\n",
    "* Word root reduction (stemming / Lemmatization)?\n",
    "* Disambuguating organism names / abbreviations? What about chemical names / abbreviations?\n",
    "* Eliminating domain-specific affixes not caught by your stemmer/lemmatizer (i.e. bacterial > bacteria)?\n",
    "* Dealing with word-internal punctuation?\n",
    "* Weighting certain terms over others (i.e. title terms vs. abstract terms)?\n",
    "* Other considerations??\n",
    "\n",
    "*Note that the data format I use for input into the SciKit Learn vectorizers is generally a Pandas series of string values, so this is the output format I'd aim for with any pre-processing setup.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = data.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out html\n",
    "txt = txt.str.replace(r'<[^<>]*>', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords\n",
    "txt = txt.str.replace(r'Background(?!\\ )', '')\n",
    "txt = txt.str.replace(r'Method(?!\\ )', '')\n",
    "txt = txt.str.replace(r'Title(?!\\ )', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually filter multigrams\n",
    "txt = txt.str.replace(r'[Qq]uantitative\\W[Tt]rait\\W[Ll]oc[i|us]+|[Qq][Tt][Ll][sS]|[Qq][Tt][Ll]', 'QTL')\n",
    "txt = txt.str.replace(r'[Ll]inkage\\W*[mM]ap\\w*', 'linkage_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pigeonpea (Cajanus cajan L. Huth) is an agronomically important legume cultivated worldwide. In this study, we extensively analyzed gene-body methylation (GbM) patterns in pigeonpea. We found a bimodal distribution of CG and CHG methylation patterns. GbM features- slow evolution rate and increased length remained conserved. Genes with moderate CG body methylation showed highest expression where as highly-methylated genes showed lowest expression. Transposable element (TE)-related genes were methylated in multiple contexts and hence classified as C-methylated genes. A low expression among C-methylated genes was associated with transposons insertion in gene-body and upstream regulatory regions. The CG methylation patterns were found to be conserved in orthologs compared with non-CG methylation. By comparing methylation patterns between differentially methylated regions (DMRs) of the three genotypes, we found that variably methylated marks are less likely to target evolutionary conserved sequences. Finally, our analysis showed enrichment of nitrogen-related genes in GbM orthologs of legumes, which could be promising candidates for generating epialleles for crop improvement.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.sample().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all species names abbreviations with species names, \n",
    "# Don't try to figure out this code\n",
    "\n",
    "def ret_match(m):\n",
    "    return m.group(1) + '_' + m.group(2) if m else np.NaN\n",
    "\n",
    "def replace_abvs(abvs, abstract):\n",
    "    for k, v in abvs:\n",
    "        abstract = re.sub(k, v, abstract)\n",
    "    return abstract\n",
    "\n",
    "abvs = txt.str.extractall('(([A-Z])\\.\\ ([a-z]\\w+))').drop_duplicates()\n",
    "abvs[3] = abvs.apply(lambda x: f'({x[1]}\\w+) ({x[2]})', axis=1)\n",
    "abvs[4] = abvs.apply(lambda x: f'({x[1]}[\\w|.]+ {x[2]})', axis=1)\n",
    "tmp = abvs.groupby(level=0).apply(lambda x: list(x[3])).to_frame().explode(0)\n",
    "tmp[1] = txt\n",
    "tmp = tmp.apply(lambda x: ret_match(re.search(x[0], x[1])), axis=1)\n",
    "tmp = pd.concat([abvs[4].groupby(level=0).apply(list).explode(), tmp], axis=1).dropna()\n",
    "tmp[1] = list(zip(tmp[4], tmp[0]))\n",
    "tmp = pd.concat([tmp[1].groupby(level=0).apply(list), txt], join='inner', axis=1)\n",
    "flt = tmp.apply(lambda x: replace_abvs(x[1], x['abstract']), axis=1)\n",
    "txt.loc[flt.index] = flt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "txt = txt.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter punctuation\n",
    "txt = txt.str.replace('-', ' ')\n",
    "txt = txt.str.replace('\\W+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split individual terms\n",
    "words = txt.str.split(' ').explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[words.str.contains('^[a-z]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = words.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = words.groupby(level=0).apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['soybean glycin max l merr is one of the most import oil and protein crop ever increas soybean consumpt necessit the improv of varieti for more effici product howev both correl among differ trait and genet interact among gene that affect a singl trait pose a challeng to soybean breed resultsto understand the genet network underli phenotyp correl we collect soybean access worldwid and phenotyp them for two year at three locat for agronom trait genom wide associ studi identifi signific genet loci among which genet interact with other loci we determin that oil synthesi relat gene are respons for fatti acid accumul in soybean and function in line with an addit model network analys demonstr that trait could be link through the linkag disequilibrium of associ loci and these link reflect phenotyp correl we reveal that loci includ the known dt1 e2 e1 ln dt2 fan and fap loci as well as undefin associ loci have pleiotrop effect on differ trait conclusionsthi studi provid insight into the genet correl among complex trait and will facilit futur soybean function studi and breed through molecular design'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.sample().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Vectorize your text data\n",
    "\n",
    "Transform your processed text into a machine-readable, numeric array. What parameters make the most sense for your data?\n",
    "\n",
    "The example vectorizer below is a [Count Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Another vectorizer commonly used with text data is a [TFIDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).  See [this site](https://scikit-learn.org/stable/modules/feature_extraction.html) for more documentation on feature extraction with SciKit Learn.\n",
    "\n",
    "Parameters used below:\n",
    "* __Maximum document frequency__. This parameter removes terms from your feature matrix that occur in more than X% of your documents. A value of .98 eliminates words that occur in more than 98% of all corpus documents. This removes noisy words that don't differentiate documents.\n",
    "* __Minimum document frequency__. This parameter removes words from your feature matrix that occur in fewer than X documents in your corpus. A value of 3 eliminates words that occur in 2 or fewer documents. This removes many unique words that don't help associate documents and reduces the dimensionality (# columns) of your matrix to improve compute times.\n",
    "* __NGram range__. A 'gram' is a term (column) in your frequency matrix, and an 'NGram' is a term made up of N tokens (individual words). Unigrams (N=1) consist of single tokens. Higher N values increase the number of tokens in direct proximity to one another that are strung together as single terms. Consider the phrase 'laser beam frequencies'. This phrase is made up of 3 unigrams ('laser', 'beam', 'frequencies'), 2 bigrams ('laser beam', 'beam frequencies'), and 1 trigram ('laser beam frequencies'). Your NGram range is the set of all N values you'd like to include in your feature matrix. A range of (1,1) only includes unigrams, while a range of (1-5) would generate features for unigrams, bigrams, trigrams, 4-grams, and 5-grams. Multigrams (higher than 1) contain important word associations that can be informative for your model in your specific domain context, but they also contain many useless phrases that serve only as noisy data. It is also important to consider that including multigrams drastically increases the dimensionality of your feature matrix and can slow down your model-generation process. While unigrams are crude, I find that they often lead to the best results.\n",
    "* __Stopwords__. Instructs the vectorizer to eliminate features matching common stop_words (the 'english' set is used below). This is helpful to catch anything you missed in pre-processing, but not required.\n",
    "\n",
    "*NOTE: The values used below are not final. You should adjust them as you see fit. Also, there are many other parameters you can use to adjust your vectorizer. Feel free to experiment with other parameters.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 739 ms, sys: 13.3 ms, total: 753 ms\n",
      "Wall time: 754 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5293x6997 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 449838 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize your vectorizer using desired paramters. Think of this step as 'flipping the on switch' and turning some knobs\n",
    "tf_vectorizer = CountVectorizer(max_df=1., # Maximum document frequency\n",
    "                                min_df=3, # Minimum document frequency\n",
    "                                max_features=None, # Limits the size of your array by only including the top N features by term frequency. Not used here (handy for huge datasets)\n",
    "                                ngram_range=(1, 1), # NGram range\n",
    "                                stop_words='english')\n",
    "\n",
    "# Create a feature representation using the vectorizer. \n",
    "# The output is a sparse matrix that can be used as input for many SciKit Learn clustering algorithms.\n",
    "# Here, 'data' represents a copy of my 'citations' dataframe. The \n",
    "# series 'txt' has been added by my preprocessing steps as a place for\n",
    "# storing the processed versions of my title / abstract text\n",
    "tf_vectorizer.fit(txt)\n",
    "features = tf_vectorizer.transform(txt)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Automatic Stopword Detection\n",
    "\n",
    "input a tf_matrix and txt and output stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_term_entropy(tf_matrix):\n",
    "    H = np.zeros(tf_matrix.shape[1])\n",
    "    \n",
    "    tf_matrix = coo_array(tf_matrix) # row col access\n",
    "    tf_wc = tf_matrix.sum(axis=0)    # TF(w, C)\n",
    "    \n",
    "    for d, w, tf in zip(tf_matrix.row, tf_matrix.col, tf_matrix.data):            \n",
    "        p_dw = tf / tf_wc[w]\n",
    "        H[w] -= p_dw * np.log2(p_dw)\n",
    "    \n",
    "    return H\n",
    "\n",
    "def calc_null_entropy(tf_vectorizer, words, doc_idx, random_rounds=5):\n",
    "    if words.shape != doc_idx.shape:\n",
    "        return None\n",
    "    \n",
    "    null_entropy = np.zeros(len(tf_vectorizer.get_feature_names()))\n",
    "    \n",
    "    random_rounds = 5\n",
    "    for i in range(0, random_rounds):\n",
    "        np.random.shuffle(words)\n",
    "        null_txt = pd.Series(words, doc_idx).groupby(level=0).apply(' '.join).values\n",
    "        null_entropy += calc_term_entropy(tf_vectorizer.transform(null_txt))\n",
    "\n",
    "    return null_entropy / random_rounds\n",
    "    \n",
    "def normalize(arr):\n",
    "    mx = arr.max()\n",
    "    mn = arr.min()\n",
    "    return (arr - mn) / (mx - mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = calc_term_entropy(features)\n",
    "vocabulary = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = txt.str.split(' ').explode()\n",
    "null = calc_null_entropy(tf_vectorizer, words.values, words.index.values)\n",
    "inform = entropy * (1 - normalize(null - entropy))\n",
    "tf = features.sum(axis=0).A[0]\n",
    "raw_downshift = (scipy.stats.rankdata(inform) - scipy.stats.rankdata(tf))\n",
    "downshift = raw_downshift / len(inform)\n",
    "semantic_entropy = inform + downshift * np.log(len(inform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.DataFrame(index=vocabulary)\n",
    "stopwords['tf'] = tf\n",
    "stopwords['entropy'] = entropy\n",
    "stopwords['information'] = inform\n",
    "stopwords['sm'] = null - entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords['downshift'] = downshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords['experimental'] = semantic_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 0\n",
    "stoptable = pd.DataFrame()\n",
    "ranktable = pd.DataFrame(index=stopwords.index)\n",
    "\n",
    "metrics = ['tf', 'entropy', 'information', 'experimental']\n",
    "for metric_name in metrics:\n",
    "    metric = stopwords[metric_name].sort_values(ascending=False)\n",
    "    stoptable[metric_name] = metric.index\n",
    "    stoptable[metric_name + '_value'] = metric.values\n",
    "    ranktable[metric_name] = stopwords[metric_name].rank(ascending=False, method='min').values.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_shift = 1 - normalize(stopwords['tf'].rank(ascending=False) - stopwords['information'].rank(ascending=False))\n",
    "tf_shift *= tf\n",
    "tf_shift = tf_shift.sort_values(ascending=False)\n",
    "stoptable['shift'] = tf_shift.index\n",
    "stoptable['shift_value'] = tf_shift.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gene              4443.446185\n",
       "soybean           3282.331841\n",
       "plant             2432.237767\n",
       "genom             2003.177206\n",
       "express           1712.562209\n",
       "                     ...     \n",
       "endosom              0.010446\n",
       "aggress              0.010446\n",
       "mesorhizobium        0.010446\n",
       "proteomexchang       0.000000\n",
       "mainstream           0.000000\n",
       "Length: 6997, dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>information</th>\n",
       "      <th>experimental</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gene</td>\n",
       "      <td>gene</td>\n",
       "      <td>thi</td>\n",
       "      <td>thi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soybean</td>\n",
       "      <td>thi</td>\n",
       "      <td>studi</td>\n",
       "      <td>studi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>result</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>genom</td>\n",
       "      <td>studi</td>\n",
       "      <td>provid</td>\n",
       "      <td>provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>express</td>\n",
       "      <td>soybean</td>\n",
       "      <td>import</td>\n",
       "      <td>import</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>chenopodium</td>\n",
       "      <td>stf1</td>\n",
       "      <td>ppo</td>\n",
       "      <td>np</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>chen</td>\n",
       "      <td>abi4</td>\n",
       "      <td>qph</td>\n",
       "      <td>al</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>chemodivers</td>\n",
       "      <td>cpk</td>\n",
       "      <td>ahl</td>\n",
       "      <td>gmfad2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>hd2</td>\n",
       "      <td>gmbzip1</td>\n",
       "      <td>aerenchyma</td>\n",
       "      <td>melatonin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>imperfect</td>\n",
       "      <td>aerenchyma</td>\n",
       "      <td>ef1α</td>\n",
       "      <td>tocopherol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6997 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tf     entropy information experimental\n",
       "0            gene        gene         thi          thi\n",
       "1         soybean         thi       studi        studi\n",
       "2           plant       plant      result       result\n",
       "3           genom       studi      provid       provid\n",
       "4         express     soybean      import       import\n",
       "...           ...         ...         ...          ...\n",
       "6992  chenopodium        stf1         ppo           np\n",
       "6993         chen        abi4         qph           al\n",
       "6994  chemodivers         cpk         ahl       gmfad2\n",
       "6995          hd2     gmbzip1  aerenchyma    melatonin\n",
       "6996    imperfect  aerenchyma        ef1α   tocopherol\n",
       "\n",
       "[6997 rows x 4 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoptable[['tf', 'entropy', 'information', 'experimental']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "stoptable[['tf', 'entropy', 'information', 'experimental']][:n].to_html('pages/stoptable.html', index=True)\n",
    "ranktable.sort_values('tf')[:n].to_html('pages/ranktable.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_mask = semantic_entropy > np.percentile(semantic_entropy, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = np.ma.compressed(np.ma.masked_array(np.array(vocabulary), mask=~stopword_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aberr' 'abil' 'abiot' 'abl' 'abov' 'abscis' 'absenc' 'absent' 'abstract'\n",
      " 'abstractsoybean' 'abstractth' 'abund' 'acceler' 'accept' 'access'\n",
      " 'accompani' 'accord' 'accordingli' 'account' 'accumul' 'accur' 'acet'\n",
      " 'achiev' 'acid' 'acquir' 'act' 'action' 'activ' 'actual' 'ad' 'adapt'\n",
      " 'add' 'addit' 'address' 'adjac' 'adjust' 'adopt' 'advanc' 'advantag'\n",
      " 'advent' 'advers' 'affect' 'agent' 'agreement' 'agricultur' 'agronom'\n",
      " 'aid' 'aim' 'allevi' 'allow' 'alon' 'alreadi' 'alter' 'altern' 'altogeth'\n",
      " 'alway' 'amen' 'america' 'amino' 'ammonia' 'amplifi' 'analys' 'analysi'\n",
      " 'analyz' 'anatom' 'ancestor' 'ancestr' 'ancient' 'ani' 'anim' 'annot'\n",
      " 'anoth' 'answer' 'anticip' 'apart' 'aphi' 'appar' 'appear' 'appli'\n",
      " 'applic' 'approach' 'approxim' 'arabidopsi' 'arabl' 'area' 'aris'\n",
      " 'arrang' 'art' 'articl' 'artifici' 'asian' 'aspect' 'assay' 'assess'\n",
      " 'assign' 'assist' 'associ' 'ataf1' 'attempt' 'attent' 'attract'\n",
      " 'attribut' 'automat' 'avail' 'avenu' 'averag' 'avoid' 'background'\n",
      " 'bacteria' 'balanc' 'base' 'basi' 'basic' 'bear' 'becam' 'becaus' 'becom'\n",
      " 'befor' 'begin' 'behavior' 'believ' 'belong' 'benefici' 'benefit' 'besid'\n",
      " 'best' 'better' 'bicolor' 'billion' 'bimolecular' 'bind' 'biochem'\n",
      " 'bioinformat' 'biolog' 'biosynthesi' 'biotechnolog' 'biotic' 'bombard'\n",
      " 'boost' 'born' 'bottleneck' 'breed' 'breeder' 'brief' 'briefli' 'bring'\n",
      " 'broad' 'broaden' 'broader' 'broadli' 'brought' 'built' 'cajan' 'cajanu'\n",
      " 'calcul' 'candid' 'capabl' 'captur' 'carboxyl' 'care' 'carri' 'case'\n",
      " 'catalyt' 'catalyz' 'categor' 'categori' 'caus' 'causal' 'cdna'\n",
      " 'cellular' 'central' 'centuri' 'cerevisia' 'certain' 'chain' 'challeng'\n",
      " 'chang' 'chapter' 'charact' 'character' 'characterist' 'chemic' 'china'\n",
      " 'chosen' 'chromatographi' 'chromosom' 'ci' 'cicer' 'clarifi' 'classif'\n",
      " 'classifi' 'clear' 'clearli' 'clone' 'close' 'closer' 'clue' 'cluster'\n",
      " 'code' 'coincid' 'coli' 'collabor' 'collect' 'coloc' 'combat' 'combin'\n",
      " 'come' 'commerci' 'commit' 'commod' 'common' 'commonli' 'commun' 'compar'\n",
      " 'comparison' 'competit' 'complement' 'complementari' 'complet' 'complex'\n",
      " 'complic' 'compon' 'compos' 'composit' 'compound' 'comprehens' 'compris'\n",
      " 'compromis' 'comput' 'concern' 'conclud' 'conclus' 'conclusionour'\n",
      " 'conclusionsa' 'conclusionsin' 'conclusionsour' 'conclusionsth'\n",
      " 'conclusionsthes' 'conclusionsthi' 'conclusionsw' 'conclusionth'\n",
      " 'conclusionthi' 'conclusionw' 'concomit' 'condit' 'conduct' 'confer'\n",
      " 'confid' 'confirm' 'conjunct' 'consecut' 'consequ' 'conserv' 'consid'\n",
      " 'consider' 'consist' 'consortium' 'constitu' 'constitut' 'constrain'\n",
      " 'constraint' 'construct' 'consum' 'consumpt' 'contain' 'content'\n",
      " 'context' 'continu' 'contrast' 'contribut' 'contributor' 'control'\n",
      " 'convent' 'convert' 'coordin' 'cope' 'core' 'correct' 'correl'\n",
      " 'correspond' 'corrobor' 'cost' 'counterpart' 'countri' 'coupl' 'cours'\n",
      " 'cover' 'creat' 'creation' 'criteria' 'critic' 'crop' 'cross' 'crucial'\n",
      " 'cue' 'cultiv' 'cultivar' 'current' 'custom' 'cut' 'cycl' 'cyst'\n",
      " 'cytolog' 'cytoplasm' 'damag' 'data' 'dataset' 'date' 'deal' 'decad'\n",
      " 'deciph' 'decreas' 'dedic' 'deduc' 'deeper' 'defect' 'defin' 'degrad'\n",
      " 'degre' 'delimit' 'delin' 'demand' 'demonstr' 'dens' 'depart' 'depend'\n",
      " 'depict' 'depth' 'deriv' 'descend' 'describ' 'descript' 'design' 'desir'\n",
      " 'despit' 'destruct' 'detect' 'determin' 'detriment' 'devast' 'develop'\n",
      " 'development' 'devis' 'did' 'diet' 'dietari' 'differ' 'differenti'\n",
      " 'difficult' 'difficulti' 'dimension' 'diminish' 'direct' 'directli'\n",
      " 'discern' 'discov' 'discoveri' 'discuss' 'diseas' 'disequilibrium'\n",
      " 'dismutas' 'display' 'dissect' 'distanc' 'distant' 'distantli' 'distinct'\n",
      " 'distinguish' 'distribut' 'disturb' 'diverg' 'divers' 'diversif' 'divid'\n",
      " 'dna' 'document' 'doe' 'domin' 'donor' 'doubl' 'downregul' 'downstream'\n",
      " 'draft' 'dramat' 'drastic' 'dri' 'drive' 'driven' 'durabl' 'dure' 'dynam'\n",
      " 'earli' 'earlier' 'easili' 'ecolog' 'econom' 'ecosystem' 'ectop' 'edibl'\n",
      " 'edu' 'effect' 'effici' 'effort' 'eighteen' 'elabor' 'electrolyt'\n",
      " 'element' 'elimin' 'elit' 'elucid' 'elus' 'emerg' 'emphas' 'emphasi'\n",
      " 'employ' 'enabl' 'encod' 'encompass' 'encyclopedia' 'end' 'endogen'\n",
      " 'endoplasm' 'energi' 'engin' 'enhanc' 'enorm' 'enrich' 'ensur' 'enter'\n",
      " 'entir' 'environ' 'environment' 'enzym' 'enzymat' 'epidermi' 'equival'\n",
      " 'escherichia' 'especi' 'essenti' 'establish' 'estim' 'ethyl' 'eukaryot'\n",
      " 'evalu' 'event' 'eventu' 'everi' 'evid' 'evidenc' 'evolut' 'evolutionari'\n",
      " 'evolutionarili' 'evolv' 'exact' 'examin' 'exampl' 'exceed' 'excel'\n",
      " 'exclud' 'exclus' 'exemplifi' 'exert' 'exhaust' 'exhibit' 'exist' 'exon'\n",
      " 'expand' 'expans' 'expect' 'expedit' 'experi' 'experienc' 'experiment'\n",
      " 'explain' 'explan' 'exploit' 'explor' 'expos' 'express' 'extend' 'extens'\n",
      " 'extent' 'extern' 'extrem' 'face' 'facilit' 'fact' 'factor' 'fail' 'fall'\n",
      " 'famili' 'far' 'farm' 'faster' 'favor' 'feasibl' 'featur' 'feed' 'field'\n",
      " 'fifti' 'final' 'fine' 'firstli' 'fix' 'flexibl' 'focu' 'focus' 'fold'\n",
      " 'follow' 'food' 'forc' 'form' 'format' 'forti' 'forward' 'foundat'\n",
      " 'fourth' 'framework' 'freeli' 'frequenc' 'frequent' 'friendli' 'fuel'\n",
      " 'fulfil' 'fulli' 'function' 'fundament' 'fungu' 'furthermor' 'fusion'\n",
      " 'futur' 'gaertn' 'gain' 'gap' 'gather' 'gene' 'gener' 'genet'\n",
      " 'geneticist' 'genom' 'genotyp' 'germplasm' 'given' 'global'\n",
      " 'glucuronidas' 'glycin' 'goal' 'good' 'govern' 'graphic' 'great'\n",
      " 'greater' 'greatest' 'greatli' 'greenhous' 'grew' 'group' 'grow' 'grown'\n",
      " 'growth' 'guid' 'ha' 'half' 'hallmark' 'hamper' 'harbor' 'health' 'help'\n",
      " 'henc' 'herb' 'herit' 'heterodera' 'heterolog' 'heterozyg' 'hidden'\n",
      " 'high' 'higher' 'highest' 'highli' 'highlight' 'hinder' 'hiseq' 'histor'\n",
      " 'histori' 'hold' 'homeostasi' 'homolog' 'hope' 'hormon' 'hous' 'howev'\n",
      " 'http' 'huge' 'human' 'hundr' 'hybrid' 'hydropon' 'hypothes' 'hypothesi'\n",
      " 'ichinoh' 'idea' 'ideal' 'ident' 'identif' 'identifi' 'illumina'\n",
      " 'illustr' 'imbal' 'immens' 'immunoprecipit' 'impact' 'impair' 'imped'\n",
      " 'implement' 'impli' 'implic' 'import' 'importantli' 'impos' 'improv'\n",
      " 'inact' 'inbr' 'incid' 'includ' 'inclus' 'incomplet' 'inconsist'\n",
      " 'incorpor' 'increas' 'increasingli' 'inde' 'independ' 'indic'\n",
      " 'indirectli' 'indispens' 'individu' 'induc' 'induct' 'industri'\n",
      " 'inexpens' 'infer' 'influenc' 'inform' 'informat' 'inher' 'inhibit'\n",
      " 'initi' 'inner' 'insight' 'instead' 'instrument' 'insuffici' 'integr'\n",
      " 'intens' 'inter' 'interact' 'interestingli' 'interf' 'interfer'\n",
      " 'intergen' 'interleukin' 'intern' 'interplay' 'interspac' 'intra'\n",
      " 'intric' 'intrigu' 'intriguingli' 'introduc' 'introduct' 'invalu'\n",
      " 'investig' 'involv' 'ioniz' 'isogen' 'isol' 'issu' 'jasmon' 'join'\n",
      " 'junction' 'just' 'kb' 'kegg' 'key' 'kind' 'kingdom' 'knock' 'know'\n",
      " 'knowledg' 'known' 'kyoto' 'label' 'laboratori' 'lack' 'laid' 'larg'\n",
      " 'larger' 'largest' 'laser' 'late' 'latest' 'lay' 'lead' 'leafi' 'leakag'\n",
      " 'leav' 'led' 'length' 'leucin' 'level' 'life' 'like' 'likelihood' 'limit'\n",
      " 'line' 'linear' 'link' 'linkag' 'linkage_map' 'liquid' 'literatur'\n",
      " 'littl' 'live' 'livestock' 'local' 'locat' 'loci' 'locu' 'long' 'longer'\n",
      " 'look' 'lose' 'loss' 'lost' 'low' 'lower' 'main' 'mainli' 'maintain'\n",
      " 'mainten' 'major' 'make' 'malondialdehyd' 'manag' 'mani' 'manipul'\n",
      " 'manner' 'map' 'markedli' 'marker' 'markov' 'mass' 'master' 'materi'\n",
      " 'matsumura' 'max' 'maxim' 'maximum' 'mda' 'mean' 'meaning' 'meanwhil'\n",
      " 'measur' 'mechan' 'mechanist' 'mediat' 'medicago' 'meet' 'meloidogyn'\n",
      " 'member' 'membran' 'merr' 'merril' 'messagea' 'messageth' 'messeng'\n",
      " 'metabol' 'methanesulfon' 'method' 'methodolog' 'microarray' 'microrna'\n",
      " 'microscopi' 'million' 'minim' 'minor' 'mitogen' 'mix' 'mock' 'model'\n",
      " 'moder' 'modern' 'modif' 'modifi' 'modul' 'molecul' 'molecular' 'monitor'\n",
      " 'monophylet' 'moreov' 'morpholog' 'mosaic' 'mostli' 'motif' 'motiv'\n",
      " 'multi' 'multigen' 'multipl' 'mutual' 'mysteri' 'narrow' 'natur' 'near'\n",
      " 'nearli' 'necessari' 'necessit' 'necrotroph' 'need' 'neg' 'net' 'network'\n",
      " 'new' 'newer' 'newli' 'nicotiana' 'nineteen' 'nomenclatur' 'non' 'normal'\n",
      " 'notabl' 'note' 'notion' 'novel' 'novo' 'nucleotid' 'nucleu' 'number'\n",
      " 'numer' 'nutrit' 'object' 'obscur' 'observ' 'obtain' 'obviou' 'occupi'\n",
      " 'occur' 'occurr' 'offer' 'onc' 'onion' 'onli' 'onlin' 'ontolog' 'open'\n",
      " 'opportun' 'oppos' 'opposit' 'optim' 'option' 'order' 'organ' 'origin'\n",
      " 'ornament' 'ortholog' 'oryza' 'otherwis' 'outlin' 'output' 'overal'\n",
      " 'overcom' 'overexpress' 'overlap' 'overview' 'owe' 'oxid' 'oxygen'\n",
      " 'paleopolyploid' 'palindrom' 'paper' 'paradigm' 'parallel' 'paramet'\n",
      " 'parent' 'partial' 'particip' 'particular' 'particularli' 'partli' 'past'\n",
      " 'path' 'pathogenesi' 'pathway' 'pattern' 'pave' 'pcr' 'peak'\n",
      " 'pentatricopeptid' 'percept' 'perfect' 'perfectli' 'perform' 'perhap'\n",
      " 'peroxid' 'perspect' 'perturb' 'phakopsora' 'pharmaceut' 'pharmacolog'\n",
      " 'phase' 'phaseolu' 'phenomenon' 'phenotyp' 'phylogenet' 'phylogeni'\n",
      " 'physicochem' 'physiolog' 'pictur' 'pinpoint' 'pivot' 'place' 'plant'\n",
      " 'planta' 'plasma' 'platform' 'play' 'player' 'plu' 'point' 'polyethylen'\n",
      " 'polymeras' 'polymorph' 'polyunsatur' 'poorli' 'popul' 'popular' 'pose'\n",
      " 'posit' 'possess' 'possibl' 'post' 'posttranscript' 'potenti' 'power'\n",
      " 'practic' 'preced' 'precis' 'predict' 'predomin' 'predominantli' 'prefer'\n",
      " 'preferenti' 'preliminari' 'prepar' 'prerequisit' 'presenc' 'present'\n",
      " 'pressur' 'presum' 'preval' 'prevent' 'previou' 'previous' 'primari'\n",
      " 'primarili' 'princip' 'principl' 'prior' 'probabl' 'problem' 'process'\n",
      " 'produc' 'product' 'profil' 'profound' 'program' 'progress' 'project'\n",
      " 'prokaryot' 'prolin' 'prolong' 'promin' 'promis' 'promot' 'prone'\n",
      " 'pronounc' 'propag' 'proper' 'properti' 'proport' 'propos' 'prospect'\n",
      " 'protect' 'protein' 'prove' 'proven' 'provid' 'pseudogen' 'public'\n",
      " 'publicli' 'publish' 'purifi' 'purpos' 'qpcr' 'qrt' 'qualit' 'qualiti'\n",
      " 'quantifi' 'quantit' 'quantiti' 'question' 'radic' 'rais' 'random'\n",
      " 'randomli' 'rang' 'rapid' 'rapidli' 'rare' 'rate' 'ratio' 'raw' 'ray'\n",
      " 'reach' 'reaction' 'reactiv' 'readili' 'real' 'realiz' 'reason'\n",
      " 'recalcitr' 'receiv' 'recent' 'recess' 'recogn' 'recognit' 'recombin'\n",
      " 'reconstruct' 'reduc' 'reduct' 'redund' 'refer' 'refin' 'regard'\n",
      " 'regardless' 'region' 'regul' 'regularli' 'regulatori' 'reinforc' 'rel'\n",
      " 'relat' 'relationship' 'releas' 'relev' 'reli' 'remain' 'remark' 'repeat'\n",
      " 'repertoir' 'report' 'repres' 'repress' 'reproduct' 'requir' 'rescu'\n",
      " 'research' 'resembl' 'resid' 'residu' 'resili' 'resolut' 'resolv'\n",
      " 'resourc' 'respect' 'respiratori' 'respond' 'respons' 'restrict' 'result'\n",
      " 'resultsa' 'resultsher' 'resultsin' 'resultsth' 'resultsto' 'resultsw'\n",
      " 'retard' 'reticulum' 'retriev' 'reveal' 'revers' 'review' 'revolution'\n",
      " 'rich' 'rise' 'rna' 'robust' 'role' 'rotat' 'roughli' 'routin'\n",
      " 'saccharomyc' 'salicyl' 'sampl' 'sativu' 'sativum' 'scale' 'scarc'\n",
      " 'scientif' 'scientist' 'scope' 'screen' 'search' 'season' 'second'\n",
      " 'secondari' 'secur' 'segment' 'segreg' 'select' 'sensit' 'separ' 'seq'\n",
      " 'sequenc' 'seri' 'seriou' 'serv' 'sessil' 'set' 'seven' 'seventeen'\n",
      " 'sever' 'share' 'shed' 'shift' 'short' 'shorter' 'shown' 'sieb' 'siebold'\n",
      " 'signal' 'signific' 'significantli' 'silico' 'similar' 'similarli'\n",
      " 'simpl' 'simultan' 'sinc' 'singl' 'site' 'situ' 'situat' 'slight'\n",
      " 'slightli' 'slower' 'small' 'smaller' 'soil' 'solid' 'solut' 'solv'\n",
      " 'sometim' 'sought' 'sourc' 'soybean' 'soysnp' 'space' 'span' 'speci'\n",
      " 'special' 'specif' 'spectrometri' 'specul' 'spite' 'split' 'spodoptera'\n",
      " 'stabil' 'stabl' 'stage' 'stapl' 'start' 'state' 'statist' 'statu'\n",
      " 'steadi' 'stearoyl' 'step' 'stimul' 'stimuli' 'stock' 'strand' 'strategi'\n",
      " 'strengthen' 'stress' 'strictli' 'strike' 'strong' 'stronger' 'strongli'\n",
      " 'structur' 'studi' 'subcellular' 'subject' 'subsequ' 'subset' 'substanti'\n",
      " 'substitut' 'subtl' 'success' 'sudden' 'suffer' 'suffici' 'suggest'\n",
      " 'suit' 'suitabl' 'summar' 'summari' 'superoxid' 'support' 'suppress'\n",
      " 'suppressor' 'surfac' 'surprisingli' 'surround' 'survey' 'surviv'\n",
      " 'suscept' 'sustain' 'symbiot' 'symbol' 'synteni' 'synthas' 'synthes'\n",
      " 'synthesi' 'systemat' 'taken' 'tandem' 'target' 'task' 'techniqu'\n",
      " 'technolog' 'tempor' 'tend' 'term' 'termin' 'test' 'thaliana' 'theoret'\n",
      " 'therebi' 'therefor' 'thi' 'thioredoxin' 'thirteen' 'thirti' 'thorough'\n",
      " 'thoroughli' 'thought' 'thousand' 'threat' 'threaten' 'throughput' 'thu'\n",
      " 'tightli' 'time' 'tissu' 'today' 'togeth' 'toll' 'took' 'tool' 'total'\n",
      " 'trace' 'track' 'tradit' 'trait' 'transcript' 'transcriptom' 'transduct'\n",
      " 'transfer' 'transmembran' 'treatment' 'tree' 'tremend' 'trend'\n",
      " 'triacylglycerol' 'trifoli' 'trigger' 'triticum' 'tune' 'turn' 'twelv'\n",
      " 'twenti' 'type' 'typic' 'ubiquit' 'ultim' 'unabl' 'uncharacter' 'unclear'\n",
      " 'uncov' 'undergo' 'undergon' 'underli' 'underpin' 'understand'\n",
      " 'understood' 'undertaken' 'underw' 'undesir' 'unequ' 'uneven' 'unevenli'\n",
      " 'unexpectedli' 'unexplor' 'unfavor' 'unfold' 'unidentifi' 'uniqu' 'unit'\n",
      " 'univers' 'unknown' 'unlik' 'unpreced' 'unravel' 'unstabl' 'untransl'\n",
      " 'unusu' 'unveil' 'updat' 'upregul' 'upstream' 'urgent' 'usa' 'use'\n",
      " 'usual' 'util' 'valid' 'valu' 'valuabl' 'vari' 'variabl' 'variat'\n",
      " 'varieti' 'variou' 'vast' 'veget' 'veri' 'verifi' 'versatil' 'version'\n",
      " 'vicin' 'view' 'vinifera' 'vital' 'vitro' 'wa' 'walp' 'warrant' 'way'\n",
      " 'weak' 'weakli' 'wealth' 'wherea' 'whilst' 'wide' 'wider' 'widespread'\n",
      " 'wild' 'window' 'work' 'world' 'worldwid' 'year' 'yeast' 'yield' 'zea'\n",
      " 'zucc']\n"
     ]
    }
   ],
   "source": [
    "    print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6542"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~stopword_mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6542"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~stopword_mask).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features @ sp.diags((~stopword_mask).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5293x6997 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 196488 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mirna         6.925958\n",
       "flood         6.084591\n",
       "scn           5.263805\n",
       "nodul         5.182777\n",
       "al            5.158064\n",
       "aphid         4.718644\n",
       "smv           4.606956\n",
       "iron          4.310724\n",
       "fe            4.288097\n",
       "rhg1          4.182601\n",
       "salt          4.161163\n",
       "cd            4.157764\n",
       "drought       4.142217\n",
       "isoflavon     4.087264\n",
       "qtl           4.053118\n",
       "methyl        4.027462\n",
       "pi            4.004814\n",
       "te            3.921734\n",
       "wrki          3.867014\n",
       "resist        3.803878\n",
       "et            3.797016\n",
       "nb            3.783909\n",
       "chickpea      3.682616\n",
       "fad2          3.666405\n",
       "lncrna        3.656203\n",
       "bean          3.649178\n",
       "mg            3.638210\n",
       "oil           3.634415\n",
       "flower        3.601925\n",
       "mapk          3.599269\n",
       "gm            3.587322\n",
       "br            3.586169\n",
       "rlk           3.578217\n",
       "peptid        3.575505\n",
       "snp           3.534400\n",
       "cle           3.530065\n",
       "nil           3.496153\n",
       "saponin       3.489578\n",
       "er            3.448505\n",
       "nf            3.446197\n",
       "pa            3.430143\n",
       "seed          3.426410\n",
       "deg           3.410346\n",
       "ltr           3.404428\n",
       "wgd           3.401785\n",
       "nitrat        3.386051\n",
       "cowpea        3.363056\n",
       "tf            3.360722\n",
       "shatter       3.360330\n",
       "shade         3.340640\n",
       "lrr           3.335076\n",
       "crispr        3.329934\n",
       "coat          3.325984\n",
       "gma           3.312522\n",
       "ugt           3.301638\n",
       "glyma         3.295639\n",
       "uv            3.288903\n",
       "e1            3.263234\n",
       "pea           3.259075\n",
       "lupin         3.254601\n",
       "germin        3.244755\n",
       "sirna         3.239343\n",
       "steril        3.228403\n",
       "wheat         3.226485\n",
       "peanut        3.226469\n",
       "toler         3.212163\n",
       "qtn           3.190720\n",
       "root          3.185261\n",
       "stress        3.180703\n",
       "cotton        3.179281\n",
       "mungbean      3.174926\n",
       "gst           3.172536\n",
       "myb           3.172014\n",
       "clock         3.170330\n",
       "senesc        3.161498\n",
       "ssr           3.156765\n",
       "alkalin       3.153422\n",
       "legum         3.146821\n",
       "edit          3.138832\n",
       "domest        3.126771\n",
       "snap          3.122427\n",
       "soja          3.106882\n",
       "duplic        3.094337\n",
       "marker        3.088271\n",
       "sa            3.084358\n",
       "tocopherol    3.079779\n",
       "linolen       3.062619\n",
       "mutant        3.053352\n",
       "potato        3.052702\n",
       "pod           3.050697\n",
       "metal         3.033242\n",
       "hair          3.029874\n",
       "sg            3.029001\n",
       "allel         3.027181\n",
       "acid          3.021356\n",
       "expansin      3.016730\n",
       "lg            3.010147\n",
       "ft            3.004194\n",
       "aba           3.000608\n",
       "tc            2.990205\n",
       "dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = entropy * (1 - normalize(raw_downshift))\n",
    "pd.Series(v, vocabulary).sort_values(ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.argsort()\n",
    "np.array(vocabulary)[v.argsort()[::-1]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ma.compressed(np.ma.masked_array(np.array(vocabulary), mask=stopword_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--------------\n",
    "## Run your algorithm\n",
    "\n",
    "It has all led up to this moment! Here's where you get to see what the algorithms make of your processed text.\n",
    "\n",
    "Here are some algorithms you can try (not exhaustive):\n",
    "* KMeans - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "* Latent Dirichlet Allocation (LDA) - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "* DBSCAN - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN\n",
    "* Nonnegative Matrix Factorization (NMF) - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html. This one is actually not a clustering algorithm either, but is designed for dimensionality reduction. However it can be leveraged for clustering in the same way that LDA can. Let me know if you want to try this one and I can go dig up some sample code for making it work\n",
    "\n",
    "|||||||||||||||||||||\n",
    "### *Notes on LDA example below*\n",
    "\n",
    "The algorithm used in the example below is called Latent Dirchlet Allocation (LDA). If you're interested, the original publication describing the algorithm is [here](https://dl.acm.org/doi/pdf/10.5555/944919.944937).\n",
    "\n",
    "LDA isn't *actually* a clustering algorithm. It maps a series of topics over a corpus of words/documents. To leverage this tool as a clustering algorithm, you need to assign documents to topics (clusters) by looking at the \"probability that a certain document belongs to a certain topic; this is based on how many words (except the current word) from this document belong to the topic of the current word.\" - See this [Towards Data Science page](https://towardsdatascience.com/latent-dirichlet-allocation-for-topic-modelling-explained-algorithm-and-python-scikit-learn-c65a82e7304d) for a good breakdown of what's going on when you implement LDA in SciKit Learn.\n",
    "\n",
    "For each document, you'll end up with a series of float values denoting probabilities for association with each topic. Using some simple code, we can 'tag' our citations in our original dataframe with the topics they've been assigned to.\n",
    "\n",
    "Parameters used below:\n",
    "\n",
    "* Number of Components (usually called K) - How many topics/clusters do you want in your model? The ideal setting for this parameter depends on the actual trends in your data, as well as how granular you want to be in your investigation. (*There is an automated method called the 'Elbow Method' for automatically determining an optimal K value for a given dataset, but your mileage may vary as clustering is not an exact science - [short description can be found here](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)*)\n",
    "* Max iterations - How many topic reassignment iterations do you want your algorithm to perform? Higher values take longer, but can lead to more coherent topics.\n",
    "* Learning method - I'm not sure about the specifics here, but 'online' (as opposed to 'batch') for this parameter lowers the compute time for larger datasets. You can read more about this with SciKit Learn's documentation.\n",
    "* Random state - LDA uses random assignments in initial passes, and passing a consistent random state to the algorithm allows you to reproduce your results over multiple function calls. More on this in the documentation if you're interestred.\n",
    "* Batch size - Again I'm not exactly sure on the specifics here because this is only used for the 'online' learning method. I think the basic idea is that you can control how many documents are being included in smaller samples pulled to make the iterations go faster.\n",
    "* Number of Jobs - This tells the algorithm how many of your computer's workers (cores) it can occupy while running. A good rule of thumb is n - 1, where n is the number of logical cores on your computer (set to 7 below because my personal laptop has 8 logical cores). Should you move your operation to an HPC environment, this parameter becomes much more important.\n",
    "\n",
    "*NOTE: The values used below are not final. You should adjust them as you see fit. Also, there are many other parameters you can use to adjust your algorithm, and each algorithm has different parameters. Feel free to experiment with other parameters.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize your algorithm using desired paramters. Think of this step as 'flipping the on switch' and turning some knobs\n",
    "lda = LatentDirichletAllocation(n_components=15,           # Number of topics\n",
    "                                  max_iter=50,               # Max learning iterations. LDA tries to solidify randomly assigned topics over many iterations, and this prevents 'run-away' operations and long compute times\n",
    "                                  learning_method='online',  # Learning method \n",
    "                                  random_state=100,          # Random state\n",
    "                                  batch_size=128,            # n docs in each learning iter\n",
    "                                  n_jobs = 7,               # Use n_jobs workers\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n",
    "# This is the operation that actually fits your model to your data\n",
    "# It returns a matrix with X columns and Y rows, where X is your # of topics/clusters ('n_components')\n",
    "# and Y is your total number of documents. Each cell contains a float probability value for a given\n",
    "# document's (row) relatedness to a given topic (column)\n",
    "%time lda_output = lda.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Sanity checks / model evaluation\n",
    "\n",
    "This is where the real finesse comes in. The goal here is to look at your resulting model and to determine how it can be improved (or if it has reached a 'good enough' state - no model will ever be perfect). This step can be similar for every corpus you work with, but the conclusions you draw and steps you take to address those conclusions will differ each time you work with a new corpus.\n",
    "\n",
    "Some questions to guide you in this step:\n",
    "* Do these topics actually make sense? Would you be able to confidently describe these results to another person?\n",
    "* Are there false equivalencies or distinctions being made? Am I getting more granular with my topics than the data actually allows? Am I grouping things that shouldn't be grouped? Why?\n",
    "* Take a look at the actual topic assignments for a few documents. What publications are actually being grouped together? Can the top/prominent words in a given cluster at least generally relate to the topic of a selected publication?\n",
    "\n",
    "There are many ways to go about this. Some general approaches I use (or hope to use in the future) for every dataset:\n",
    "\n",
    "* What are the top words in each of your clusters? Are there similarities? Is noisy/artifactual data showing up?\n",
    "* How many documents out of your corpus are assigned to each cluster? Are they all being lumped into 2-3 big topics while the smaller topics only get tiny numbers of documents? Are they more evenly distributed?\n",
    "* Do your resulting topics seem too broad? Too specific? You may need to adjust your n_components parameter.\n",
    "* Try visualizing your clustering results. The methods for doing so differ based on the output of each algorithm, so you may need to get creative with your code (LDA example below)\n",
    "* Are there any quantitative measures you can use to evaluate your clustering results? One example is the ['Silhouette Method' for KMeans models](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html), which attempts to determine distance separation between clusters. This method can tell you if a larger or smaller K value might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||||||||\n",
    "\n",
    "### *Surface Level - documents in topics / terms in topics*\n",
    "\n",
    "Below is some sample code for A) assigning your documents to clusters, B) looking at the most prominent words in a given topic, and C) seeing how many documents are in a given cluster. I usually do this as a first sanity check for LDA output before sending my results to pyLDAvis.\n",
    "\n",
    "The first step is to assign your original citations to clusters/topics based on your model's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe where the index (Y) is the docs, and the columns (X) are the topics\n",
    "# Assign \"clusters\" based on highest topic association probability for each doc\n",
    "\n",
    "# First step: create a blank dataframe with K columns and N rows, where K is the number of topics (n_components)\n",
    "# and N is your total number of documents\n",
    "topics = [str(i) for i in range(lda.n_components)]\n",
    "docs = [str(i) for i in range(data.shape[0])]\n",
    "\n",
    "# Populate this blank dataframe using your LDA output. What you're doing is here is translating the\n",
    "# raw lda output (a numpy.ndarray of numpy.ndarrays) into the rows/columns you established above\n",
    "document_topic_df = pd.DataFrame(np.round(lda_output, 2), columns=topics, index=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use Numpy's argmax function to assign documents to topics. What's happening here is that the function is examining all of the probability values in a given row, and picking the column (topic) with the highest probability for a given row (document). Unfortunately, when there is a tie, [np.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) picks the 'first' highest value it finds. If you want to instead randomize topic assignments in the case of ties, you might need to improvise (a good Stack Overflow thread on that can be found [here](https://stackoverflow.com/questions/42071597/numpy-argmax-random-tie-breaking)).\n",
    "\n",
    "The code below essentially returns a Numpy ndarray of highest-probability topics for all of your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(document_topic_df.values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, your topic assignments ndarray above is in the same order as your original dataframe, so you can simply add those assignments as a new column to your 'citations' dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By 'labels' - we're referring to topics\n",
    "citations['label'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to see how many documents are in each topic. This is a nice baseline look at your document distribution across your model. One nasty habit I see with a lot of LDA models I build is that the lion's share of documents will be placed in a select few topics, while the remaining topics will be very small and meaningless. This is generally a sign that you need to make some adjustments.\n",
    "\n",
    "The code below uses the Pandas value_counts() function on the 'labels' column of your citations data frame to make a new dataframe, where one column is your topic #, and the other is your document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = pd.DataFrame(citations['label'].value_counts()).reset_index()\n",
    "ranks.index += 1\n",
    "ranks.columns = ['topic', 'docs']\n",
    "all_ranks = ranks.sort_values(by=\"topic\").reset_index(drop=True)\n",
    "all_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can see where your documents have landed, let's take a look at what words are most prominent in each topic. If you see nonsense or noise showing up as 'prominent' in a given topic, it's probably time for some further tuning.\n",
    "\n",
    "The code below makes use of a function - 'show_topics' - that I found online (can't remember the link). I don't remember exactly what's going on, but suffice it to say that the function takes your model, vectorizer, and the # of words you want to see as input, and returns a list of ndarrays containing top terms, one for each topic, as output. In the example below, I chose to return the top 15 terms for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(tf_vectorizer, lda, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can translate that list of ndarrays into a dataframe for easy viewing. The code below does so and also assigns some cleaner column/row names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll find situations in which certain topics don't get any documents assigned to them. We'll want to see those too, so the code below adds them to your topic dataframe. It also adds the # of documents assigned to each topic, as determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ranks = ranks.sort_values(by=\"topic\").reset_index(drop=True)\n",
    "clusters = 15\n",
    "\n",
    "for i in range(0, clusters):\n",
    "    if i not in all_ranks['topic'].values:\n",
    "        all_ranks = pd.concat([all_ranks, pd.DataFrame([{'topic':i, 'docs':0}])])\n",
    "\n",
    "df_topic_keywords['docs'] = list(all_ranks['docs'])\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||||||||\n",
    "\n",
    "### *LDA Visualization Example*\n",
    "\n",
    "I use PyLDAvis extensively every time I use LDA. It's a really handy way to get an overhead view of your terms/clusters and the relationships between them. You can read more about how it works [here](https://pyldavis.readthedocs.io/en/latest/readme.html).\n",
    "\n",
    "[This article on PyLDAvis](https://towardsdatascience.com/topic-model-visualization-using-pyldavis-fecd7c18fbf6) is was paywalled for me, but I think you can access it with a free account. You may get some useful tips from it. [This notebook](https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&lambda=1&term=), linked on the package documentation site, has a lot of handy code examples and descriptions.\n",
    "\n",
    "Arguments used below:\n",
    "\n",
    "* 'lda' is the variable for your initialized algorithm (not the results)\n",
    "* 'features' is the variable for the sparse matrix returned by your vectorizer\n",
    "* 'tf_vectorizer' if the variable for your initialized vectorizer\n",
    "* 'mds' designates your multidimensional scaling method. Because the resulting visualization is displayed on a 2D plane, you have to reduce your multi-dimensional feature to something you can actually look at. There are several options here, and I don't know the specifics on how they differ, but 'tsne' has always worked well for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your visualization.\n",
    "panel = pyLDAvis.sklearn.prepare(lda, features, tf_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inline display that shows up directly in your Notebook\n",
    "pyLDAvis.display(panel)\n",
    "\n",
    "\n",
    "\n",
    "# Note that if you use this command with JupyterLab, all of your Jupyter UI buttons will disappear.\n",
    "# The alternative is pyLDAvis.show(), which opens an HTML version of the visual in a new tab.\n",
    "# Unfortunately the most recent version of pyLDAvis has an issue with its\n",
    "# html generation, so it's probably safer to use .display() with Jupyter Notebooks instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyLDAvis.save_html(panel, 'pages/lemmatized.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, abstract in enumerate(citations.abstract[txt.str.contains('biofuel')].sample()):\n",
    "    with open(f'sample_texts/abstract_{i}.txt', 'w') as f:\n",
    "        print(abstract)\n",
    "        f.write(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Future Directions\n",
    "\n",
    "If you get to where you are happy with your model, there are plenty of other directions in which you could take your results.\n",
    "\n",
    "We can talk specifics on any of these (or anything you come up with on your own) when we get there, but here are some starter thoughts:\n",
    "\n",
    "* Dynamic topic visualization. Could we build something that would allow folks to interactively explore the model you've built? What are some subtopics in each major cluster? Where are the authors of papers in that cluster coming from? What are some notable examples of papers/outcomes that are grouped in a given model?\n",
    "* Connecting these models with community analysis for authors on the publications. Can we break down the authors on these papers into subcommunities? How do those subcommunities correspond or overlap with the topics/clusters from your model? Could we connect a topic visualization with a network visualization?\n",
    "* Prediction. Could we explore some applications involving supervised learning on your processed data? What classes within these publications might be worth predicting? Publishing journals? Citation thresholds? High vs. low media attention?\n",
    "* Change over time. How does your model look if we add in a temporal element? Are certain topics gaining/losing prominence over time?\n",
    "* Proposal analysis. What does it look like if we throw the original Soybean proposal text into the mix? Can we get some idea of which papers are closely aligned with the original proposal and which papers are not?\n",
    "* More publications. How widely applicable is your model? If I provided a larger, more general set of publications dealing heavily with soybean research, would you need to adjust your parameter tuning or preprocessing steps? If not, how close are the topics/clusters in the larger topic to those you generated just from those papers citing JGI's soybean genome?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
