{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbba302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from scipy.sparse import coo_array, csr_array, csc_array, csr_matrix, coo_matrix, csc_matrix\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ac944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_junk(txt):\n",
    "    # html\n",
    "    txt = txt.str.replace(r'<[^<>]*>', '', regex=True)\n",
    "\n",
    "    # paper structure\n",
    "    txt = txt.str.replace(r'Background(?!\\ )', '', regex=True)\n",
    "    txt = txt.str.replace(r'Method(?!\\ )', '', regex=True)\n",
    "    txt = txt.str.replace(r'Title(?!\\ )', '', regex=True)\n",
    "    return txt \n",
    "\n",
    "def replace_abvs(abstract, abv_map):\n",
    "        if not isinstance(abv_map, dict):\n",
    "            return abstract\n",
    "\n",
    "        for key in abv_map.keys():\n",
    "            abstract = abstract.replace(key, abv_map[key])\n",
    "        return abstract\n",
    "\n",
    "def replace_species_abbreviations(txt):\n",
    "    # find all abbreviations of the form ' P. suffix' => 'Prefix Suffix' (note leading whitespace)\n",
    "    abvs = txt.str.findall(r'\\s([A-Z]\\.\\ [a-z]\\w*)')\n",
    "    abvs = abvs.apply(lambda x: x if x != [] else np.NaN).dropna()\n",
    "\n",
    "    # make new dataframe of abv and abstract\n",
    "    abv_df = pd.concat([txt, abvs], axis=1).dropna()\n",
    "    \n",
    "    \n",
    "    abv_df.columns.values[0] = \"abstract\"\n",
    "    abv_df.columns.values[1] = \"abv\"\n",
    "    \n",
    "    \n",
    "    abv_df = abv_df.explode('abv').drop_duplicates()\n",
    "\n",
    "    # split by prefix and suffix\n",
    "    split_abv = abv_df.abv.str.split('. ')\n",
    "    \n",
    "    abv_df['prefix'] = split_abv.apply(lambda x: x[0])\n",
    "    abv_df['suffix'] = split_abv.apply(lambda x: x[-1])\n",
    "\n",
    "    # match by suffix\n",
    "    # drop all abbreviations without exactly one unique full prefix\n",
    "    abv_df['matches'] = abv_df.apply(lambda x: set(re.findall(f'(\\w+)\\s+{x.suffix}', x.abstract)), axis=1)\n",
    "    abv_df = abv_df[abv_df.apply(lambda x: len(x.matches) == 1, axis=1)]\n",
    "    abv_df['matches'] = abv_df.matches.apply(lambda x: list(x)[0])\n",
    "\n",
    "    # filter out any matches that don't have same starting letter as prefix\n",
    "    abv_df = abv_df[abv_df.matches.str[0] == abv_df.prefix]\n",
    "\n",
    "    # unabbreviate\n",
    "    abv_df['unabbv'] = abv_df.matches + ' '+ abv_df.suffix\n",
    "    abv_df['connected'] = abv_df.matches + '_' + abv_df.suffix\n",
    "    abv_df = abv_df.drop(columns=['prefix', 'suffix', 'matches'])\n",
    "    abv_df\n",
    "\n",
    "    abstract_group = abv_df.groupby(abv_df.index)\n",
    "    abv_mappings = abstract_group.apply(lambda x: x.set_index('abv').to_dict()['unabbv'])\n",
    "    connect_mappings = abstract_group.apply(lambda x: x.set_index('unabbv').to_dict()['connected'])\n",
    "    abstracts = abstract_group.apply(lambda x: x.abstract.iloc[0])\n",
    "    abv_map = pd.concat([txt, abv_mappings, connect_mappings], axis=1, keys=['abstract', 'abv_map', 'con_map'])\n",
    "\n",
    "    removed_abvs = abv_map.apply(lambda x: replace_abvs(x.abstract, x.abv_map), axis=1)\n",
    "    abv_map['abstract'] = removed_abvs\n",
    "    connected_abvs = abv_map.apply(lambda x: replace_abvs(x.abstract, x.con_map), axis=1)\n",
    "    \n",
    "    return connected_abvs\n",
    "\n",
    "def txt_to_words(txt):\n",
    "    return txt.str.split('[\\W+|-]').explode()\n",
    "\n",
    "def words_to_txt(words):\n",
    "    return words.groupby(level=0).apply(' '.join)\n",
    "\n",
    "def make_words_df(txt):\n",
    "    txt = txt.dropna()\n",
    "    txt = clean_junk(txt)\n",
    "    txt = replace_species_abbreviations(txt)\n",
    "    stemmer = PorterStemmer()\n",
    "    words = txt_to_words(txt)\n",
    "    unique_words = words.dropna().unique()\n",
    "    df = pd.DataFrame(unique_words, columns=['plain'])\n",
    "    df['stem'] = df.plain.apply(stemmer.stem)\n",
    "    stem_map = df.set_index('plain').stem.to_dict()\n",
    "    sentences = txt.str.split('\\.').explode().dropna()\n",
    "    words_df = sentences.str.split('\\ +').reset_index().explode(column=0).reset_index()\n",
    "    words_df = words_df.rename(columns={'level_0': 'sentence', 'index': 'doc', 0: 'words'})\n",
    "    words_df['stems'] = words_df['words'].map(stem_map, 'ignore')\n",
    "    words_df = words_df.dropna()\n",
    "    words_df = words_df[words_df.stems.str.contains('^[a-zA-Z]+')]\n",
    "    return words_df\n",
    "\n",
    "def get_sentences(words_df):\n",
    "    sentences = words_df.groupby('sentence').stems.apply(' '.join)\n",
    "    return sentences\n",
    "\n",
    "def get_docs(words_df):\n",
    "    docs = words_df.groupby('doc').stems.apply(' '.join)\n",
    "    return docs\n",
    "\n",
    "def calc_term_entropy(tf_matrix):\n",
    "    H = np.zeros(tf_matrix.shape[1])\n",
    "    \n",
    "    tf_matrix = coo_array(tf_matrix) # row col access\n",
    "    tf_wc = tf_matrix.sum(axis=0) # TF(w, C)\n",
    "    \n",
    "    for d, w, tf in zip(tf_matrix.row, tf_matrix.col, tf_matrix.data):\n",
    "        p_dw = tf / tf_wc[w]\n",
    "        H[w] -= p_dw * np.log2(p_dw)\n",
    "    return H\n",
    "\n",
    "def get_stopwords(tf_matrix, vocabulary, random_rounds=10):\n",
    "    entropy = calc_term_entropy(tf_docs)\n",
    "    entropy = pd.Series(entropy, vocabulary, name='entropy', dtype='float64')\n",
    "    \n",
    "    null_entropy = np.zeros(vocabulary.shape[0])\n",
    "\n",
    "    for i in range(0, random_rounds):    \n",
    "        words_df['null'] = words_df.stems.sample(frac=1).to_numpy()\n",
    "        null_docs = words_df.groupby('doc').null.apply(' '.join) \n",
    "        tf_null      = tf_vectorizer.transform(null_docs)\n",
    "        null_entropy += calc_term_entropy(tf_null)\n",
    "\n",
    "    null_entropy = null_entropy / random_rounds\n",
    "    \n",
    "    stopwords = pd.DataFrame(entropy, columns=['entropy'])\n",
    "    stopwords['tf'] = words_df.stems.value_counts()\n",
    "    stopwords = stopwords[['tf', 'entropy']]\n",
    "    stopwords[f'null'] = null_entropy\n",
    "    stopwords['infor'] = null_entropy - stopwords.entropy\n",
    "    \n",
    "def drop_stopwords(words_df, stopword_list):\n",
    "    stopwords = set(stopwords_list)\n",
    "    return words_df[~words_df.stems.isin(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260dc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = pd.read_csv('all_soybean_citations.csv')\n",
    "data = citations.copy()\n",
    "txt = data.abstract.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e558f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = clean_junk(txt)\n",
    "txt = replace_species_abbreviations(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b433908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Heat stress driven by global warming has affec...\n",
       "1       The combination of apomixis and hybrid product...\n",
       "2       The zinc deficiency response in Arabidopsis_th...\n",
       "3       E3 - ubiquitin ligases are known to confer abi...\n",
       "4       Pod borer , Helicoverpa armigera , a polyphagu...\n",
       "                              ...                        \n",
       "5395    Transposable elements are the most abundant co...\n",
       "5397    The Soybean Consensus Map 4_0 facilitated the ...\n",
       "5398    Soybean somatic embryos have attracted attenti...\n",
       "5399    The number and distribution of branches in soy...\n",
       "5400    The generation of useful mutant alleles of spe...\n",
       "Length: 5293, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = txt.str.replace('(\\d+)\\.(\\d+)', '\\\\1_\\\\2', regex=True)\n",
    "txt = txt.str.replace('([^\\w| ]) *', ' \\\\1 ', regex=True)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25664881",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = txt_to_words(txt)\n",
    "unique_words = words.dropna().unique()\n",
    "df = pd.DataFrame(unique_words, columns=['plain'])\n",
    "df['stem'] = df.plain.apply(stemmer.stem)\n",
    "stem_map = df.set_index('plain').stem.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54d919b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>doc</th>\n",
       "      <th>words</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heat</td>\n",
       "      <td>heat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stress</td>\n",
       "      <td>stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>driven</td>\n",
       "      <td>driven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>global</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441576</th>\n",
       "      <td>59293</td>\n",
       "      <td>5400</td>\n",
       "      <td>genetic</td>\n",
       "      <td>genet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441577</th>\n",
       "      <td>59293</td>\n",
       "      <td>5400</td>\n",
       "      <td>diversity</td>\n",
       "      <td>divers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441578</th>\n",
       "      <td>59293</td>\n",
       "      <td>5400</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441579</th>\n",
       "      <td>59293</td>\n",
       "      <td>5400</td>\n",
       "      <td>polyploid</td>\n",
       "      <td>polyploid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441580</th>\n",
       "      <td>59293</td>\n",
       "      <td>5400</td>\n",
       "      <td>crops</td>\n",
       "      <td>crop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1150956 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence   doc      words      stems\n",
       "0               0     0       Heat       heat\n",
       "1               0     0     stress     stress\n",
       "2               0     0     driven     driven\n",
       "3               0     0         by         by\n",
       "4               0     0     global     global\n",
       "...           ...   ...        ...        ...\n",
       "1441576     59293  5400    genetic      genet\n",
       "1441577     59293  5400  diversity     divers\n",
       "1441578     59293  5400         of         of\n",
       "1441579     59293  5400  polyploid  polyploid\n",
       "1441580     59293  5400      crops       crop\n",
       "\n",
       "[1150956 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = txt.str.split('\\.').explode().dropna()\n",
    "words_df = sentences.str.split('\\ +').reset_index().explode(column=0).reset_index()\n",
    "words_df = words_df.rename(columns={'level_0': 'sentence', 'index': 'doc', 0: 'words'})\n",
    "words_df['stems'] = words_df['words'].map(stem_map, 'ignore')\n",
    "words_df = words_df.dropna()\n",
    "words_df = words_df[words_df.stems.str.contains('^[a-zA-Z]+')]\n",
    "\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e8ae2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = words_df.groupby('sentence').stems.apply(' '.join)\n",
    "docs = words_df.groupby('doc').stems.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c122bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=1.,\n",
    "                                min_df=3,\n",
    "                                max_features=None,\n",
    "                                ngram_range=(1, 1), \n",
    "                                stop_words=None\n",
    "                                )\n",
    "\n",
    "tf_vectorizer.fit(docs)\n",
    "\n",
    "tf_sentences = tf_vectorizer.transform(sentences)\n",
    "tf_docs      = tf_vectorizer.transform(docs)\n",
    "\n",
    "\n",
    "vocabulary   = tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea879216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_df = pd.DataFrame.sparse.from_spmatrix(tf_sentences, columns=vocabulary)\n",
    "# docs_df = pd.DataFrame.sparse.from_spmatrix(tf_docs, columns=vocabulary)\n",
    "\n",
    "tf_sentences = coo_array(tf_sentences)\n",
    "tf_docs = coo_array(tf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e87a37",
   "metadata": {},
   "source": [
    "$$w = \\text{term}$$\n",
    "$$d = \\text{document}$$\n",
    "$$C = \\text{corpus}$$\n",
    "\n",
    "$$k = \\text{randomization rounds}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e49c4",
   "metadata": {},
   "source": [
    "Shannon Word Entropy formula:\n",
    "$$p(d|w) = \\frac{TF(w, d)}{TF(w, C)}$$\n",
    "\n",
    "$$H(w, C) = - \\sum_{d}p(d|w)\\log(p(d|w))$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44dc344",
   "metadata": {},
   "source": [
    "My formula (Modified Infor):\n",
    "$$S(C) = \\text{randomize}(C)$$\n",
    "\n",
    "$$\\overline{H}(w, C) = \\sum_{k}\\frac{H(w, S(C))}{k}$$\n",
    "\n",
    "$$r(w, C) = \\overline{H}(w, C) - H(w, C)$$\n",
    "\n",
    "$$\\hat{r}(w, C) = \\text{normalize}(r(w,C))$$\n",
    "\n",
    "$$I(w, C) = H(w, C)(1 - \\hat{r}(w, C))$$\n",
    "\n",
    "$$d(w, C) = \\frac{rank(I(w, C)) - rank(TF(w, C))}{|\\{w\\}|}$$\n",
    "\n",
    "$$SE(w, C) = I(w, C) + d(w) * log_2(|\\{w\\}|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f2140d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_term_entropy(tf_matrix):\n",
    "    H = np.zeros(tf_matrix.shape[1])\n",
    "    tf_matrix = coo_array(tf_matrix) # row col access\n",
    "    tf_wc = tf_matrix.sum(axis=0) # TF(w, C)\n",
    "    for d, w, tf in zip(tf_matrix.row, tf_matrix.col, tf_matrix.data):\n",
    "        p_dw = tf / tf_wc[w]\n",
    "        H[w] -= p_dw * np.log2(p_dw)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52fa40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighboring_tf(m):\n",
    "    m = csr_matrix(m)\n",
    "    bool_m = m > 0\n",
    "    sums = (bool_m.transpose() * m)\n",
    "    m = csr_matrix(m)\n",
    "    bool_m = m > 0\n",
    "    sums = (bool_m.transpose() * m)\n",
    "    inv_totals = csr_matrix(1 / m.sum(axis=0))\n",
    "    norms = sums.multiply(inv_totals)\n",
    "    norms.data = (1 / norms.data)\n",
    "    return norms.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a18e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = calc_term_entropy(tf_docs)\n",
    "entropy = pd.Series(entropy, vocabulary, name='entropy', dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14059da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n",
      "[2.5849625 0.        0.        ... 1.        2.        3.169925 ]\n"
     ]
    }
   ],
   "source": [
    "null_entropy = np.zeros(vocabulary.shape[0])\n",
    "random_rounds = 12\n",
    "\n",
    "for i in range(0, random_rounds):    \n",
    "    words_df['null'] = words_df.stems.sample(frac=1).to_numpy()\n",
    "    null_docs = words_df.groupby('doc').null.apply(' '.join) \n",
    "    tf_null      = tf_vectorizer.transform(null_docs)\n",
    "    null_entropy += calc_term_entropy(tf_null)\n",
    "    \n",
    "null_entropy = null_entropy / random_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81612b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.DataFrame(entropy, columns=['entropy'])\n",
    "stopwords['tf'] = words_df.stems.value_counts()\n",
    "stopwords = stopwords[['tf', 'entropy']]\n",
    "stopwords[f'null'] = null_entropy\n",
    "stopwords['infor'] = null_entropy - stopwords.entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4090476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([[4, 0, 0],\n",
    "              [1, 8, 0],\n",
    "              [0, 1, 1],\n",
    "              [0, 2, 3],\n",
    "              [4, 0, 0]])\n",
    "\n",
    "m = csr_matrix(m)\n",
    "bool_m = m > 0\n",
    "sums = (bool_m.transpose() * m)\n",
    "totals = csr_matrix(1 / m.sum(axis=0))\n",
    "norms = sums.multiply(totals)\n",
    "corr = (norms.sum(axis=1) / bool_m.transpose().sum(axis=1)).A.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "752da490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.72727273, 0.        ],\n",
       "       [0.11111111, 1.        , 1.        ],\n",
       "       [0.        , 0.27272727, 1.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93048bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = neighboring_tf(tf_sentences).A.flatten()\n",
    "stopwords['corr'] = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9c72b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>null</th>\n",
       "      <th>infor</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a0</th>\n",
       "      <td>6</td>\n",
       "      <td>1.792481</td>\n",
       "      <td>2.584963</td>\n",
       "      <td>0.792481</td>\n",
       "      <td>470.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a02</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a03</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a04</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a05</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyd6</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyd7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zygomorph</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zygomorphi</th>\n",
       "      <td>4</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>372.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zygot</th>\n",
       "      <td>9</td>\n",
       "      <td>2.725481</td>\n",
       "      <td>3.169925</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>595.685714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23308 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tf   entropy      null     infor        corr\n",
       "a0           6  1.792481  2.584963  0.792481  470.900000\n",
       "a02          1  0.000000  0.000000  0.000000   24.000000\n",
       "a03          1  0.000000  0.000000  0.000000   24.000000\n",
       "a04          1  0.000000  0.000000  0.000000   24.000000\n",
       "a05          1  0.000000  0.000000  0.000000   24.000000\n",
       "...         ..       ...       ...       ...         ...\n",
       "zyd6         4  0.000000  2.000000  2.000000  165.000000\n",
       "zyd7         2  0.000000  1.000000  1.000000   40.000000\n",
       "zygomorph    2  1.000000  1.000000  0.000000  125.000000\n",
       "zygomorphi   4  1.500000  2.000000  0.500000  372.666667\n",
       "zygot        9  2.725481  3.169925  0.444444  595.685714\n",
       "\n",
       "[23308 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e251e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "infor = (stopwords.null - stopwords.entropy)\n",
    "infor_norm = 1 - (infor - infor.min()) / (infor.max() - infor.min())\n",
    "stopwords['experimental'] = stopwords.entropy * infor_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10a45b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "i_thresh = 1\n",
    "\n",
    "top_tf = stopwords.tf.sort_values(ascending=False)[:n]\n",
    "top_entropy = stopwords.entropy.sort_values(ascending=False).astype('float16')[:n]\n",
    "top_infor = stopwords.assign(intropy= .5 * stopwords.entropy // 1, abs_infor=abs(stopwords.infor)\n",
    "                             ).sort_values(by=['intropy', 'infor'], ascending=(False,True)\n",
    "                             ).infor.astype('float16')[:n]\n",
    "top_infor_sorted = stopwords.infor.sort_values().astype('float16')[:n]\n",
    "top_post_tf = stopwords[abs(stopwords.infor) > i_thresh].sort_values('tf', ascending=False)[:n].tf\n",
    "top_corr = stopwords['corr'].sort_values(ascending=False).astype('float16')[:n]\n",
    "top_null = stopwords.null.sort_values(ascending=False).astype('float16')[:n]\n",
    "top_experimental = stopwords.experimental.sort_values(ascending=False).astype('float16')[:n * 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "815bd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tf.to_csv('stopwords/tf.tsv', sep='\\t')\n",
    "top_entropy.to_csv('stopwords/entropy.tsv', sep='\\t')\n",
    "top_infor.to_csv('stopwords/infor.tsv', sep='\\t')\n",
    "top_infor_sorted.to_csv('stopwords/infor_sorted.tsv', sep='\\t')\n",
    "top_post_tf.to_csv('stopwords/post.tsv', sep='\\t')\n",
    "top_corr.to_csv('stopwords/corr.tsv', sep='\\t')\n",
    "top_null.to_csv('stopwords/null.tsv', sep='\\t')\n",
    "top_experimental.to_csv('stopwords/experimental.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b06283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr = top_corr / top_corr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36c454d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3268598836.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [40]\u001b[0;36m\u001b[0m\n\u001b[0;31m    experimental =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "infor = (stopwords.null - stopwords.entropy)\n",
    "infor_norm = 1 - (infor - infor.min()) / (infor.max() - infor.min())\n",
    "# experimental = (stopwords.entropy * 2 // 1) + infor_norm\n",
    "# experimental = stopwords.null + stopwords.entropy\n",
    "\n",
    "# experimental = stopwords[stopwords.entropy > (stopwords.entropy.max() * .75)].infor\n",
    "\n",
    "experimental = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846d8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f4aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d03f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = pd.DataFrame()\n",
    "ranks['tf'] = stopwords.tf\n",
    "ranks['entropy_rank'] = stopwords.entropy.rank(method='first', ascending=False).astype('int')\n",
    "ranks['experimental_rank'] = stopwords.experimental.rank(method='first', ascending=False).astype('int')\n",
    "ranks['tf_rank'] = stopwords.tf.rank(method='first', ascending=False).astype('int')\n",
    "ranks = ranks.sort_values('tf', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26712d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks.to_html('ranks.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks['downshift'] = (ranks.experimental_rank - ranks.entropy_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks[ranks.entropy_rank < ranks.shape[0]].sort_values('downshift', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b126b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "downshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff31b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e3f80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ranks[ranks.tf_rank < ranks.shape[0] * .2].sort_values('downshift', ascending=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0eb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ranks.downshift\n",
    "\n",
    "ds_norm = 2 * (ds - ds.min()) / (ds.max() - ds.min()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde63fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental - ds_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbabb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
