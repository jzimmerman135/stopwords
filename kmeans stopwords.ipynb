{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e8232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.cluster as cl\n",
    "\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.sparse import coo_array, csr_array, csc_array, csr_matrix, coo_matrix, csc_matrix\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8181afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_junk(txt):\n",
    "    # html\n",
    "    txt = txt.str.replace(r'<[^<>]*>', '', regex=True)\n",
    "\n",
    "    # paper structure\n",
    "    txt = txt.str.replace(r'Background(?!\\ )', '', regex=True)\n",
    "    txt = txt.str.replace(r'Method(?!\\ )', '', regex=True)\n",
    "    txt = txt.str.replace(r'Title(?!\\ )', '', regex=True)\n",
    "    return txt \n",
    "\n",
    "def replace_abvs(abstract, abv_map):\n",
    "        if not isinstance(abv_map, dict):\n",
    "            return abstract\n",
    "\n",
    "        for key in abv_map.keys():\n",
    "            abstract = abstract.replace(key, abv_map[key])\n",
    "        return abstract\n",
    "\n",
    "def replace_species_abbreviations(txt):\n",
    "    # find all abbreviations of the form ' P. suffix' => 'Prefix Suffix' (note leading whitespace)\n",
    "    abvs = txt.str.findall(r'\\s([A-Z]\\.\\ [a-z]\\w*)')\n",
    "    abvs = abvs.apply(lambda x: x if x != [] else np.NaN).dropna()\n",
    "\n",
    "    # make new dataframe of abv and abstract\n",
    "    abv_df = pd.concat([txt, abvs], axis=1).dropna()\n",
    "\n",
    "    abv_df.columns.values[0] = \"abstract\"\n",
    "    abv_df.columns.values[1] = \"abv\"\n",
    "    \n",
    "    abv_df = abv_df.explode('abv').drop_duplicates()\n",
    "\n",
    "    # split by prefix and suffix\n",
    "    split_abv = abv_df.abv.str.split('. ')\n",
    "    \n",
    "    abv_df['prefix'] = split_abv.apply(lambda x: x[0])\n",
    "    abv_df['suffix'] = split_abv.apply(lambda x: x[-1])\n",
    "\n",
    "    # match by suffix\n",
    "    # drop all abbreviations without exactly one unique full prefix\n",
    "    abv_df['matches'] = abv_df.apply(lambda x: set(re.findall(f'(\\w+)\\s+{x.suffix}', x.abstract)), axis=1)\n",
    "    abv_df = abv_df[abv_df.apply(lambda x: len(x.matches) == 1, axis=1)]\n",
    "    abv_df['matches'] = abv_df.matches.apply(lambda x: list(x)[0])\n",
    "\n",
    "    # filter out any matches that don't have same starting letter as prefix\n",
    "    abv_df = abv_df[abv_df.matches.str[0] == abv_df.prefix]\n",
    "\n",
    "    # unabbreviate\n",
    "    abv_df['unabbv'] = abv_df.matches + ' '+ abv_df.suffix\n",
    "    abv_df['connected'] = abv_df.matches + '_' + abv_df.suffix\n",
    "    abv_df = abv_df.drop(columns=['prefix', 'suffix', 'matches'])\n",
    "    abv_df\n",
    "\n",
    "    abstract_group = abv_df.groupby(abv_df.index)\n",
    "    abv_mappings = abstract_group.apply(lambda x: x.set_index('abv').to_dict()['unabbv'])\n",
    "    connect_mappings = abstract_group.apply(lambda x: x.set_index('unabbv').to_dict()['connected'])\n",
    "    abstracts = abstract_group.apply(lambda x: x.abstract.iloc[0])\n",
    "    abv_map = pd.concat([txt, abv_mappings, connect_mappings], axis=1, keys=['abstract', 'abv_map', 'con_map'])\n",
    "\n",
    "    removed_abvs = abv_map.apply(lambda x: replace_abvs(x.abstract, x.abv_map), axis=1)\n",
    "    abv_map['abstract'] = removed_abvs\n",
    "    connected_abvs = abv_map.apply(lambda x: replace_abvs(x.abstract, x.con_map), axis=1)\n",
    "    \n",
    "    return connected_abvs\n",
    "\n",
    "def txt_to_words(txt):\n",
    "    return txt.str.split('[\\W+|-]').explode()\n",
    "\n",
    "def words_to_txt(words):\n",
    "    return words.groupby(level=0).apply(' '.join)\n",
    "\n",
    "def make_words_df(txt):\n",
    "    txt = txt.dropna()\n",
    "    txt = clean_junk(txt)\n",
    "    txt = replace_species_abbreviations(txt)\n",
    "    stemmer = PorterStemmer()\n",
    "    words = txt_to_words(txt)\n",
    "    unique_words = words.dropna().unique()\n",
    "    df = pd.DataFrame(unique_words, columns=['plain'])\n",
    "    df['stem'] = df.plain.apply(stemmer.stem)\n",
    "    stem_map = df.set_index('plain').stem.to_dict()\n",
    "    sentences = txt.str.split('\\.').explode().dropna()\n",
    "    words_df = sentences.str.split('\\ +').reset_index().explode(column=0).reset_index()\n",
    "    words_df = words_df.rename(columns={'level_0': 'sentence', 'index': 'doc', 0: 'words'})\n",
    "    words_df['stems'] = words_df['words'].map(stem_map, 'ignore')\n",
    "    words_df = words_df.dropna()\n",
    "    words_df = words_df[words_df.stems.str.contains('^[a-zA-Z]+')]\n",
    "    return words_df\n",
    "\n",
    "def get_sentences(words_df):\n",
    "    sentences = words_df.groupby('sentence').stems.apply(' '.join)\n",
    "    return sentences\n",
    "\n",
    "def get_docs(words_df):\n",
    "    docs = words_df.groupby('doc').stems.apply(' '.join)\n",
    "    return docs\n",
    "\n",
    "def calc_term_entropy(tf_matrix):\n",
    "    H = np.zeros(tf_matrix.shape[1])\n",
    "    \n",
    "    tf_matrix = coo_array(tf_matrix) # row col access\n",
    "\n",
    "    tf_wc = tf_matrix.sum(axis=0) # TF(w, C)\n",
    "    \n",
    "    for d, w, tf in zip(tf_matrix.row, tf_matrix.col, tf_matrix.data):\n",
    "        p_dw = tf / tf_wc[w]\n",
    "        H[w] -= p_dw * np.log2(p_dw)\n",
    "    return H\n",
    "\n",
    "def get_stopwords(tf_matrix, vocabulary, random_rounds=10):\n",
    "    entropy = calc_term_entropy(tf_docs)\n",
    "    entropy = pd.Series(entropy, vocabulary, name='entropy', dtype='float64')\n",
    "    \n",
    "    null_entropy = np.zeros(vocabulary.shape[0])\n",
    "\n",
    "    for i in range(0, random_rounds):    \n",
    "        words_df['null'] = words_df.stems.sample(frac=1).to_numpy()\n",
    "        null_docs = words_df.groupby('doc').null.apply(' '.join) \n",
    "        tf_null      = tf_vectorizer.transform(null_docs)\n",
    "        null_entropy += calc_term_entropy(tf_null)\n",
    "\n",
    "    null_entropy = null_entropy / random_rounds\n",
    "    \n",
    "    stopwords = pd.DataFrame(entropy, columns=['entropy'])\n",
    "    stopwords['tf'] = words_df.stems.value_counts()\n",
    "    stopwords = stopwords[['tf', 'entropy']]\n",
    "    stopwords[f'null'] = null_entropy\n",
    "    stopwords['infor'] = null_entropy - stopwords.entropy\n",
    "    return stopwords\n",
    "    \n",
    "def drop_stopwords(words_df, stopword_list):\n",
    "    stopwords = set(stopword_list)\n",
    "    return words_df[~words_df.stems.isin(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1c32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = pd.read_csv('all_soybean_citations.csv')\n",
    "citations = citations[~citations.abstract.isna() & ~citations.title.isna()]\n",
    "\n",
    "data = citations.copy()\n",
    "txt = data.abstract.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf0dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = make_words_df(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc821cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc\n",
       "0       heat stress driven by global warm ha affect fo...\n",
       "1       the combin of apomixi and hybrid product is ha...\n",
       "2       the zinc defici respons in arabidopsis_thalian...\n",
       "3       ligas are known to confer abiot stress respons...\n",
       "4       pod helicoverpa a polyphagu herbivor caus exte...\n",
       "                              ...                        \n",
       "5395    transpos element are the most abund compon of ...\n",
       "5397    the soybean consensu map facilit the anchor of...\n",
       "5398    soybean somat embryo have attract attent both ...\n",
       "5399    the number and distribut of branch in soybean ...\n",
       "5400    the gener of use mutant allel of specif gene w...\n",
       "Name: stems, Length: 5289, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = get_docs(words_df)\n",
    "sentences = get_sentences(words_df)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c501ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=1.,\n",
    "                                min_df=3,\n",
    "                                max_features=None,\n",
    "                                ngram_range=(1, 1), \n",
    "                                stop_words=None\n",
    "                                )\n",
    "\n",
    "tf_vectorizer.fit(docs)\n",
    "\n",
    "tf_sentences = tf_vectorizer.transform(sentences)\n",
    "tf_docs      = tf_vectorizer.transform(docs)\n",
    "\n",
    "\n",
    "vocabulary   = tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a739daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stopwords(tf_docs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e59216c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "of               12.200256\n",
       "and              12.198789\n",
       "the              12.177180\n",
       "in               12.145572\n",
       "to               12.065804\n",
       "that             11.758386\n",
       "for              11.702673\n",
       "with             11.699265\n",
       "is               11.695638\n",
       "gene             11.569620\n",
       "thi              11.568888\n",
       "by               11.440028\n",
       "we               11.370369\n",
       "were             11.346004\n",
       "plant            11.333840\n",
       "soybean          11.311824\n",
       "as               11.278272\n",
       "are              11.252206\n",
       "on               11.250976\n",
       "from             11.240391\n",
       "these            11.207005\n",
       "use              11.203264\n",
       "wa               11.186198\n",
       "identifi         11.153365\n",
       "an               11.142512\n",
       "be               11.125165\n",
       "studi            11.068710\n",
       "which            11.031241\n",
       "express          10.975331\n",
       "result           10.941949\n",
       "have             10.926000\n",
       "analysi          10.906385\n",
       "import           10.844907\n",
       "function         10.785224\n",
       "develop          10.784750\n",
       "provid           10.746665\n",
       "genom            10.734361\n",
       "show             10.680590\n",
       "it               10.605270\n",
       "been             10.576871\n",
       "differ           10.563545\n",
       "sequenc          10.551376\n",
       "protein          10.550506\n",
       "also             10.549487\n",
       "two              10.519103\n",
       "their            10.493600\n",
       "role             10.480565\n",
       "ha               10.478072\n",
       "at               10.453165\n",
       "genet            10.435865\n",
       "respons          10.433714\n",
       "or               10.429985\n",
       "includ           10.382372\n",
       "reveal           10.358817\n",
       "involv           10.342747\n",
       "into             10.335246\n",
       "our              10.332125\n",
       "between          10.305785\n",
       "suggest          10.302368\n",
       "regul            10.253418\n",
       "one              10.211422\n",
       "most             10.201438\n",
       "other            10.191490\n",
       "transcript       10.177992\n",
       "molecular        10.173022\n",
       "indic            10.159125\n",
       "stress           10.139241\n",
       "increas          10.131305\n",
       "improv           10.128505\n",
       "crop             10.118286\n",
       "not              10.109479\n",
       "both             10.098448\n",
       "compar           10.090435\n",
       "but              10.078738\n",
       "associ           10.058419\n",
       "can              10.036341\n",
       "speci            10.014165\n",
       "level            10.012881\n",
       "major            10.009452\n",
       "found             9.980102\n",
       "famili            9.956803\n",
       "mechan            9.955524\n",
       "than              9.920982\n",
       "may               9.913828\n",
       "among             9.904359\n",
       "play              9.899566\n",
       "present           9.899540\n",
       "further           9.894616\n",
       "more              9.894014\n",
       "understand        9.872752\n",
       "sever             9.867878\n",
       "all               9.867188\n",
       "data              9.862031\n",
       "base              9.856104\n",
       "such              9.849280\n",
       "under             9.840718\n",
       "high              9.839023\n",
       "three             9.838157\n",
       "breed             9.835570\n",
       "region            9.830125\n",
       "trait             9.826832\n",
       "select            9.826039\n",
       "character         9.780921\n",
       "factor            9.761807\n",
       "dure              9.745209\n",
       "control           9.744462\n",
       "product           9.690465\n",
       "process           9.653398\n",
       "potenti           9.648478\n",
       "relat             9.640100\n",
       "report            9.637433\n",
       "analys            9.635373\n",
       "gener             9.633424\n",
       "through           9.631763\n",
       "new               9.608854\n",
       "quantit           9.600904\n",
       "well              9.587455\n",
       "seed              9.578351\n",
       "line              9.576137\n",
       "activ             9.573340\n",
       "effect            9.573052\n",
       "pattern           9.567334\n",
       "number            9.560263\n",
       "growth            9.555842\n",
       "arabidopsi        9.552336\n",
       "contain           9.547069\n",
       "will              9.543993\n",
       "chromosom         9.512463\n",
       "mani              9.511498\n",
       "specif            9.506321\n",
       "total             9.505458\n",
       "key               9.498676\n",
       "conserv           9.486234\n",
       "map               9.485499\n",
       "root              9.475822\n",
       "encod             9.465693\n",
       "pathway           9.455363\n",
       "divers            9.451381\n",
       "investig          9.446019\n",
       "candid            9.435084\n",
       "highli            9.429851\n",
       "signific          9.426873\n",
       "could             9.421891\n",
       "max               9.420907\n",
       "significantli     9.409844\n",
       "known             9.408360\n",
       "onli              9.406403\n",
       "acid              9.398569\n",
       "larg              9.391685\n",
       "variou            9.389064\n",
       "perform           9.378149\n",
       "structur          9.369557\n",
       "popul             9.367578\n",
       "yield             9.363209\n",
       "differenti        9.355536\n",
       "phenotyp          9.354698\n",
       "chang             9.340149\n",
       "detect            9.338199\n",
       "demonstr          9.328165\n",
       "novel             9.327320\n",
       "singl             9.313168\n",
       "while             9.308616\n",
       "inform            9.306240\n",
       "group             9.301404\n",
       "some              9.299996\n",
       "resist            9.269067\n",
       "interact          9.267998\n",
       "recent            9.266027\n",
       "phylogenet        9.263018\n",
       "research          9.255283\n",
       "marker            9.251052\n",
       "higher            9.249820\n",
       "enhanc            9.237377\n",
       "approach          9.236758\n",
       "cultivar          9.232390\n",
       "condit            9.226087\n",
       "affect            9.219857\n",
       "toler             9.210863\n",
       "analyz            9.197238\n",
       "multipl           9.191667\n",
       "four              9.182960\n",
       "respect           9.182212\n",
       "genotyp           9.181678\n",
       "contribut         9.179183\n",
       "abiot             9.169108\n",
       "similar           9.148071\n",
       "biolog            9.141941\n",
       "loci              9.138805\n",
       "within            9.137201\n",
       "induc             9.125068\n",
       "model             9.118246\n",
       "signal            9.109624\n",
       "avail             9.107107\n",
       "reduc             9.102207\n",
       "observ            9.101879\n",
       "identif           9.085216\n",
       "legum             9.085059\n",
       "resourc           9.079585\n",
       "evolut            9.072208\n",
       "target            9.072166\n",
       "Name: entropy, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.entropy.sort_values(ascending=False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b621d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_thresh = .4\n",
    "\n",
    "infor = stopwords.infor\n",
    "\n",
    "\n",
    "# stopwords_set = set(infor[abs(infor) < i_thresh].index)\n",
    "# len(stopwords_set)\n",
    "\n",
    "stopwords_set = stopwords.entropy.sort_values(ascending=False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1141fa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abiot',\n",
       " 'acid',\n",
       " 'activ',\n",
       " 'affect',\n",
       " 'all',\n",
       " 'also',\n",
       " 'among',\n",
       " 'an',\n",
       " 'analys',\n",
       " 'analysi',\n",
       " 'analyz',\n",
       " 'and',\n",
       " 'approach',\n",
       " 'arabidopsi',\n",
       " 'are',\n",
       " 'as',\n",
       " 'associ',\n",
       " 'at',\n",
       " 'avail',\n",
       " 'base',\n",
       " 'be',\n",
       " 'been',\n",
       " 'between',\n",
       " 'biolog',\n",
       " 'both',\n",
       " 'breed',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'candid',\n",
       " 'chang',\n",
       " 'character',\n",
       " 'chromosom',\n",
       " 'compar',\n",
       " 'condit',\n",
       " 'conserv',\n",
       " 'contain',\n",
       " 'contribut',\n",
       " 'control',\n",
       " 'could',\n",
       " 'crop',\n",
       " 'cultivar',\n",
       " 'data',\n",
       " 'demonstr',\n",
       " 'detect',\n",
       " 'develop',\n",
       " 'differ',\n",
       " 'differenti',\n",
       " 'divers',\n",
       " 'dure',\n",
       " 'effect',\n",
       " 'encod',\n",
       " 'enhanc',\n",
       " 'evolut',\n",
       " 'express',\n",
       " 'factor',\n",
       " 'famili',\n",
       " 'for',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'function',\n",
       " 'further',\n",
       " 'gene',\n",
       " 'gener',\n",
       " 'genet',\n",
       " 'genom',\n",
       " 'genotyp',\n",
       " 'group',\n",
       " 'growth',\n",
       " 'ha',\n",
       " 'have',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highli',\n",
       " 'identif',\n",
       " 'identifi',\n",
       " 'import',\n",
       " 'improv',\n",
       " 'in',\n",
       " 'includ',\n",
       " 'increas',\n",
       " 'indic',\n",
       " 'induc',\n",
       " 'inform',\n",
       " 'interact',\n",
       " 'into',\n",
       " 'investig',\n",
       " 'involv',\n",
       " 'is',\n",
       " 'it',\n",
       " 'key',\n",
       " 'known',\n",
       " 'larg',\n",
       " 'legum',\n",
       " 'level',\n",
       " 'line',\n",
       " 'loci',\n",
       " 'major',\n",
       " 'mani',\n",
       " 'map',\n",
       " 'marker',\n",
       " 'max',\n",
       " 'may',\n",
       " 'mechan',\n",
       " 'model',\n",
       " 'molecular',\n",
       " 'more',\n",
       " 'most',\n",
       " 'multipl',\n",
       " 'new',\n",
       " 'not',\n",
       " 'novel',\n",
       " 'number',\n",
       " 'observ',\n",
       " 'of',\n",
       " 'on',\n",
       " 'one',\n",
       " 'onli',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'pathway',\n",
       " 'pattern',\n",
       " 'perform',\n",
       " 'phenotyp',\n",
       " 'phylogenet',\n",
       " 'plant',\n",
       " 'play',\n",
       " 'popul',\n",
       " 'potenti',\n",
       " 'present',\n",
       " 'process',\n",
       " 'product',\n",
       " 'protein',\n",
       " 'provid',\n",
       " 'quantit',\n",
       " 'recent',\n",
       " 'reduc',\n",
       " 'region',\n",
       " 'regul',\n",
       " 'relat',\n",
       " 'report',\n",
       " 'research',\n",
       " 'resist',\n",
       " 'resourc',\n",
       " 'respect',\n",
       " 'respons',\n",
       " 'result',\n",
       " 'reveal',\n",
       " 'role',\n",
       " 'root',\n",
       " 'seed',\n",
       " 'select',\n",
       " 'sequenc',\n",
       " 'sever',\n",
       " 'show',\n",
       " 'signal',\n",
       " 'signific',\n",
       " 'significantli',\n",
       " 'similar',\n",
       " 'singl',\n",
       " 'some',\n",
       " 'soybean',\n",
       " 'speci',\n",
       " 'specif',\n",
       " 'stress',\n",
       " 'structur',\n",
       " 'studi',\n",
       " 'such',\n",
       " 'suggest',\n",
       " 'target',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'these',\n",
       " 'thi',\n",
       " 'three',\n",
       " 'through',\n",
       " 'to',\n",
       " 'toler',\n",
       " 'total',\n",
       " 'trait',\n",
       " 'transcript',\n",
       " 'two',\n",
       " 'under',\n",
       " 'understand',\n",
       " 'use',\n",
       " 'variou',\n",
       " 'wa',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'which',\n",
       " 'while',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'yield']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = list(stopwords_set.index)\n",
    "sorted(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a2d144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'drought' in stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c5bbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc\n",
       "0       heat driven global warm forest surviv a larger...\n",
       "1       combin apomixi hybrid hail holi grail agricult...\n",
       "2       zinc defici arabidopsis_thaliana basic there e...\n",
       "3       ligas confer a isol possess ligas aba treatmen...\n",
       "4       pod helicoverpa a polyphagu herbivor caus exte...\n",
       "                              ...                        \n",
       "5395    transpos element abund compon eukaryot documen...\n",
       "5397    consensu facilit anchor whole joint depart den...\n",
       "5398    somat embryo attract attent a zygot embryo exp...\n",
       "5399    distribut branch influenc effici light util lo...\n",
       "5400    mutant allel would acceler convent program com...\n",
       "Name: stems, Length: 5289, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs = get_docs(drop_stopwords(words_df, stopwords_set.index))\n",
    "filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a4f990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords_by_entropy(tf_matrix, n_stopwords):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a5798b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cl.KMeans(n_clusters=3, random_state=0).fit(tf_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9577d372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5008\n",
       "0     280\n",
       "2       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(kmeans.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a309c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heat stress driven by global warm ha affect fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the combin of apomixi and hybrid product is ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the zinc defici respons in arabidopsis_thalian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ligas are known to confer abiot stress respons...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pod helicoverpa a polyphagu herbivor caus exte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5284</th>\n",
       "      <td>transpos element are the most abund compon of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5285</th>\n",
       "      <td>the soybean consensu map facilit the anchor of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5286</th>\n",
       "      <td>soybean somat embryo have attract attent both ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>the number and distribut of branch in soybean ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5288</th>\n",
       "      <td>the gener of use mutant allel of specif gene w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5289 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  label\n",
       "0     heat stress driven by global warm ha affect fo...      1\n",
       "1     the combin of apomixi and hybrid product is ha...      1\n",
       "2     the zinc defici respons in arabidopsis_thalian...      1\n",
       "3     ligas are known to confer abiot stress respons...      1\n",
       "4     pod helicoverpa a polyphagu herbivor caus exte...      1\n",
       "...                                                 ...    ...\n",
       "5284  transpos element are the most abund compon of ...      1\n",
       "5285  the soybean consensu map facilit the anchor of...      1\n",
       "5286  soybean somat embryo have attract attent both ...      1\n",
       "5287  the number and distribut of branch in soybean ...      1\n",
       "5288  the gener of use mutant allel of specif gene w...      1\n",
       "\n",
       "[5289 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data={'abstract': docs.reset_index(drop=True), 'label': kmeans.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6db5cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calc_term_entropy(tf_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f6ee49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a1          2.584963\n",
       "a17         2.521641\n",
       "a2          4.297079\n",
       "a5          1.405639\n",
       "aa          3.027169\n",
       "              ...   \n",
       "zone        4.729438\n",
       "zp          2.000000\n",
       "zucc        5.129283\n",
       "zyd00006    2.807355\n",
       "zygot       2.500000\n",
       "Length: 5996, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(results, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d0a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
